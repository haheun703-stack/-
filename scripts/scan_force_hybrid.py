"""ì„¸ë ¥ê°ì§€ í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ìºë„ˆ â€” 3ì¸µ í†µí•© ë¶„ì„

3ê°œ ì‹œìŠ¤í…œì„ í†µí•©í•˜ì—¬ ì„¸ë ¥ê°ì§€ íƒ­ì— ê³µê¸‰í•©ë‹ˆë‹¤:

  Layer 1. ìˆ˜ê¸‰ê±´ì „ì„± (ë§¤í¬ë¡œ) â€” ì‹œì¥ ì „ì²´ ìê¸ˆ íë¦„ ê±´ì „ì„±
    - ì™¸ì¸/ê¸°ê´€ ìˆœë§¤ìˆ˜ ë™í–¥ (ìµœê·¼ 5ì¼)
    - KOSPI ëŒ€ë¹„ ìˆ˜ê¸‰ ì˜¨ë„
    - ì¢…í•© ê²½ë³´ ë“±ê¸‰: ì•ˆì „ / ì£¼ì˜ / ìœ„í—˜

  Layer 2. ì´ìƒê±°ë˜ íƒì§€ (ë§ˆì´í¬ë¡œ) â€” ê¸°ì¡´ 5íŒ¨í„´ + VWAP ê°•í™”
    - P1 ê±°ë˜ëŸ‰í­ë°œ, P2 ìˆ˜ê¸‰ë°˜ì „, P3 BBìŠ¤í€´ì¦ˆ
    - P4 OBVë‹¤ì´ë²„ì „ìŠ¤, P5 ìˆ˜ê¸‰ì´íƒˆ
    - + P6 VWAP ì´íƒˆ (ë‹¹ì¼ VWAP ëŒ€ë¹„ ì¢…ê°€ ê´´ë¦¬)
    - + P7 ì—°ì† ê¸°ê´€/ì™¸ì¸ ë§¤ì§‘ (5ì¼+ ì—°ì† ìˆœë§¤ìˆ˜)

  Layer 3. ì´ë²¤íŠ¸ ë ˆì´ë” (ë©”ì¡°) â€” RSS ë‰´ìŠ¤ + DART ê³µì‹œ ê¸°ë°˜ ì´ë²¤íŠ¸ ê°ì§€
    - crawl_market_newsì˜ high/medium ì„íŒ©íŠ¸ ë‰´ìŠ¤
    - DART ì „ìê³µì‹œ (ë‰´ìŠ¤ ëŒ€ë¹„ 30ë¶„~ìˆ˜ì‹œê°„ ì„ í–‰)
    - theme_dictionary í‚¤ì›Œë“œ ë§¤ì¹­ â†’ ìˆ˜í˜œì¢…ëª© ì—°ê²°

ì¶œë ¥: data/force_hybrid.json

Usage:
    python scripts/scan_force_hybrid.py
"""

from __future__ import annotations

import argparse
import json
import logging
import sys
from datetime import datetime, timedelta
from pathlib import Path

import numpy as np
import pandas as pd

PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

logger = logging.getLogger(__name__)

PROCESSED_DIR = PROJECT_ROOT / "data" / "processed"
CSV_DIR = PROJECT_ROOT / "stock_data_daily"
KOSPI_PATH = PROJECT_ROOT / "data" / "kospi_index.csv"
MARKET_NEWS_PATH = PROJECT_ROOT / "data" / "market_news.json"
DART_PATH = PROJECT_ROOT / "data" / "dart_disclosures.json"
THEME_DICT_PATH = PROJECT_ROOT / "config" / "theme_dictionary.yaml"
OUTPUT_PATH = PROJECT_ROOT / "data" / "force_hybrid.json"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìœ í‹¸ë¦¬í‹°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _sf(val, default=0):
    """NaN/Inf ì•ˆì „ ë³€í™˜"""
    try:
        v = float(val)
        return default if (np.isnan(v) or np.isinf(v)) else round(v, 2)
    except (TypeError, ValueError):
        return default


def build_name_map() -> dict[str, str]:
    """CSV íŒŒì¼ëª…ì—ì„œ ì¢…ëª©ì½”ë“œ â†’ ì¢…ëª©ëª… ë§¤í•‘"""
    name_map = {}
    for csv in CSV_DIR.glob("*.csv"):
        parts = csv.stem.rsplit("_", 1)
        if len(parts) == 2:
            name, ticker = parts
            name_map[ticker] = name
    return name_map


def get_flow_from_csv(ticker: str) -> dict:
    """CSVì—ì„œ ìµœê·¼ ìˆ˜ê¸‰ ìƒì„¸ ë°ì´í„° ì¶”ì¶œ"""
    csvs = list(CSV_DIR.glob(f"*_{ticker}.csv"))
    if not csvs:
        return {}
    try:
        df = pd.read_csv(csvs[0], parse_dates=["Date"])
        df = df.dropna(subset=["Close"]).sort_values("Date")
        if len(df) < 10:
            return {}
        tail = df.tail(10)
        f_net = tail["Foreign_Net"].values if "Foreign_Net" in tail.columns else np.zeros(10)
        i_net = tail["Inst_Net"].values if "Inst_Net" in tail.columns else np.zeros(10)
        return {
            "f_prev5": float(np.nansum(f_net[:5])),
            "f_last5": float(np.nansum(f_net[5:])),
            "i_prev5": float(np.nansum(i_net[:5])),
            "i_last5": float(np.nansum(i_net[5:])),
            "f_today": float(f_net[-1]) if len(f_net) > 0 else 0,
            "i_today": float(i_net[-1]) if len(i_net) > 0 else 0,
            # ì—°ì† ìˆœë§¤ìˆ˜ ì¼ìˆ˜
            "f_streak": _count_streak(f_net),
            "i_streak": _count_streak(i_net),
        }
    except Exception:
        return {}


def _count_streak(arr) -> int:
    """ìµœê·¼ë¶€í„° ì—°ì† ì–‘ìˆ˜(ìˆœë§¤ìˆ˜) ì¼ìˆ˜"""
    streak = 0
    for v in reversed(arr):
        if np.isnan(v) or v <= 0:
            break
        streak += 1
    return streak


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Layer 1: ìˆ˜ê¸‰ê±´ì „ì„± (ì‹œì¥ ë§¤í¬ë¡œ)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def analyze_supply_demand_health() -> dict:
    """ì‹œì¥ ì „ì²´ ìˆ˜ê¸‰ ê±´ì „ì„± ë¶„ì„"""
    # ì „ ì¢…ëª© CSVì—ì„œ ì™¸ì¸/ê¸°ê´€ ìˆ˜ê¸‰ ì§‘ê³„
    total_foreign_5d = 0
    total_inst_5d = 0
    total_foreign_today = 0
    total_inst_today = 0
    foreign_buying_count = 0  # ì™¸ì¸ ìˆœë§¤ìˆ˜ ì¢…ëª© ìˆ˜
    inst_buying_count = 0     # ê¸°ê´€ ìˆœë§¤ìˆ˜ ì¢…ëª© ìˆ˜
    total_stocks = 0

    for pq in PROCESSED_DIR.glob("*.parquet"):
        try:
            df = pd.read_parquet(pq, columns=["close", "volume", "ì™¸êµ­ì¸í•©ê³„", "ê¸°ê´€í•©ê³„"])
            if len(df) < 5:
                continue
            tail = df.tail(5)
            close = float(tail["close"].iloc[-1])
            vol = float(tail["volume"].iloc[-1])
            if close < 1000 or vol < 1000:
                continue

            total_stocks += 1
            f_5d = float(np.nansum(tail["ì™¸êµ­ì¸í•©ê³„"].values)) if "ì™¸êµ­ì¸í•©ê³„" in tail.columns else 0
            i_5d = float(np.nansum(tail["ê¸°ê´€í•©ê³„"].values)) if "ê¸°ê´€í•©ê³„" in tail.columns else 0
            f_today = float(tail["ì™¸êµ­ì¸í•©ê³„"].iloc[-1]) if "ì™¸êµ­ì¸í•©ê³„" in tail.columns else 0
            i_today = float(tail["ê¸°ê´€í•©ê³„"].iloc[-1]) if "ê¸°ê´€í•©ê³„" in tail.columns else 0

            total_foreign_5d += f_5d
            total_inst_5d += i_5d
            total_foreign_today += f_today
            total_inst_today += i_today

            if f_5d > 0:
                foreign_buying_count += 1
            if i_5d > 0:
                inst_buying_count += 1

        except Exception:
            continue

    # KOSPI ë™í–¥
    kospi_info = _get_kospi_trend()

    # ê±´ì „ì„± ì ìˆ˜ ê³„ì‚° (0~100)
    score = 50  # ê¸°ë³¸ ì¤‘ë¦½

    # ì™¸ì¸ ìˆ˜ê¸‰ (Â±20)
    if total_stocks > 0:
        f_buy_ratio = foreign_buying_count / total_stocks
        score += (f_buy_ratio - 0.5) * 40  # 50% ê¸°ì¤€ Â±20

    # ê¸°ê´€ ìˆ˜ê¸‰ (Â±15)
    if total_stocks > 0:
        i_buy_ratio = inst_buying_count / total_stocks
        score += (i_buy_ratio - 0.5) * 30  # 50% ê¸°ì¤€ Â±15

    # KOSPI ì¶”ì„¸ (Â±15)
    if kospi_info.get("above_ma20"):
        score += 10
    if kospi_info.get("above_ma60"):
        score += 5
    if kospi_info.get("pct_5d", 0) > 1:
        score += 5
    elif kospi_info.get("pct_5d", 0) < -1:
        score -= 5

    score = max(0, min(100, score))

    # ê²½ë³´ ë“±ê¸‰
    if score >= 65:
        alert = "ì•ˆì „"
        alert_color = "#10B981"
        alert_desc = "ìˆ˜ê¸‰ ì–‘í˜¸ â€” ì™¸ì¸/ê¸°ê´€ ìœ ì… ìš°ì„¸"
    elif score >= 40:
        alert = "ì£¼ì˜"
        alert_color = "#F59E0B"
        alert_desc = "ìˆ˜ê¸‰ í˜¼ì¡° â€” ë§¤ìˆ˜/ë§¤ë„ í˜¼ì¬"
    else:
        alert = "ìœ„í—˜"
        alert_color = "#EF4444"
        alert_desc = "ìˆ˜ê¸‰ ì•…í™” â€” ì™¸ì¸/ê¸°ê´€ ì´íƒˆ ìš°ì„¸"

    return {
        "score": round(score, 1),
        "alert": alert,
        "alert_color": alert_color,
        "alert_desc": alert_desc,
        "total_stocks": total_stocks,
        "foreign": {
            "net_5d": round(total_foreign_5d),
            "net_today": round(total_foreign_today),
            "buying_count": foreign_buying_count,
            "buying_ratio": round(foreign_buying_count / max(total_stocks, 1) * 100, 1),
        },
        "institution": {
            "net_5d": round(total_inst_5d),
            "net_today": round(total_inst_today),
            "buying_count": inst_buying_count,
            "buying_ratio": round(inst_buying_count / max(total_stocks, 1) * 100, 1),
        },
        "kospi": kospi_info,
    }


def _get_kospi_trend() -> dict:
    """KOSPI ì§€ìˆ˜ ì¶”ì„¸"""
    if not KOSPI_PATH.exists():
        return {}
    try:
        df = pd.read_csv(KOSPI_PATH, parse_dates=["Date"])
        df = df.sort_values("Date").tail(60)
        if len(df) < 20:
            return {}
        last = df.iloc[-1]
        close = float(last["Close"])
        ma20 = float(df["Close"].tail(20).mean())
        ma60 = float(df["Close"].tail(60).mean()) if len(df) >= 60 else ma20

        pct_1d = (close / float(df.iloc[-2]["Close"]) - 1) * 100 if len(df) >= 2 else 0
        pct_5d = (close / float(df.iloc[-6]["Close"]) - 1) * 100 if len(df) >= 6 else 0

        return {
            "close": round(close, 2),
            "ma20": round(ma20, 2),
            "ma60": round(ma60, 2),
            "above_ma20": close > ma20,
            "above_ma60": close > ma60,
            "pct_1d": round(pct_1d, 2),
            "pct_5d": round(pct_5d, 2),
        }
    except Exception:
        return {}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Layer 2: ì´ìƒê±°ë˜ íƒì§€ (ê¸°ì¡´ í™•ì¥)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def detect_volume_explosion(df: pd.DataFrame) -> dict | None:
    """P1: ê±°ë˜ëŸ‰ í­ë°œ"""
    if "volume_surge_ratio" not in df.columns:
        return None
    last = df.iloc[-1]
    vsr = float(last["volume_surge_ratio"])
    if np.isnan(vsr) or vsr < 2.5:
        return None
    pct = float(last.get("price_change", 0)) if "price_change" in df.columns else 0
    direction = "ìƒìŠ¹í­ë°œ" if pct > 1.0 else ("í•˜ë½í­ë°œ" if pct < -1.0 else "íš¡ë³´í­ë°œ")
    return {
        "pattern": "P1_ê±°ë˜ëŸ‰í­ë°œ",
        "strength": min(vsr / 3.0, 3.0),
        "vsr": round(vsr, 1),
        "direction": direction,
        "price_change": _sf(pct),
        "desc": f"ê±°ë˜ëŸ‰ {vsr:.1f}ë°° í­ë°œ ({direction})",
    }


def detect_flow_reversal(df: pd.DataFrame, csv_flow: dict) -> dict | None:
    """P2: ìˆ˜ê¸‰ ë°˜ì „"""
    if not csv_flow:
        if "ì™¸êµ­ì¸í•©ê³„" not in df.columns:
            return None
        tail = df.tail(10)
        if len(tail) < 10:
            return None
        f_vals = tail["ì™¸êµ­ì¸í•©ê³„"].values
        i_vals = tail["ê¸°ê´€í•©ê³„"].values if "ê¸°ê´€í•©ê³„" in tail.columns else np.zeros(10)
        f_prev5, f_last5 = float(np.nansum(f_vals[:5])), float(np.nansum(f_vals[5:]))
        i_prev5, i_last5 = float(np.nansum(i_vals[:5])), float(np.nansum(i_vals[5:]))
        f_today = float(f_vals[-1]) if not np.isnan(f_vals[-1]) else 0
        i_today = float(i_vals[-1]) if not np.isnan(i_vals[-1]) else 0
    else:
        f_prev5, f_last5 = csv_flow["f_prev5"], csv_flow["f_last5"]
        i_prev5, i_last5 = csv_flow["i_prev5"], csv_flow["i_last5"]
        f_today, i_today = csv_flow["f_today"], csv_flow["i_today"]

    reasons, strength = [], 0
    if f_prev5 < 0 and f_last5 > 0 and abs(f_last5) > abs(f_prev5) * 0.5:
        reasons.append(f"ì™¸ì¸ ë§¤ë„â†’ë§¤ìˆ˜ ë°˜ì „ ({f_prev5:+,.0f}â†’{f_last5:+,.0f})")
        strength += 1.5
    if i_prev5 < 0 and i_last5 > 0 and abs(i_last5) > abs(i_prev5) * 0.5:
        reasons.append(f"ê¸°ê´€ ë§¤ë„â†’ë§¤ìˆ˜ ë°˜ì „ ({i_prev5:+,.0f}â†’{i_last5:+,.0f})")
        strength += 1.5
    if f_today > 0 and i_today > 0:
        reasons.append(f"ê¸ˆì¼ ì™¸ì¸({f_today:+,.0f})+ê¸°ê´€({i_today:+,.0f}) ë™ì‹œë§¤ìˆ˜")
        strength += 0.5
    if strength < 1.0:
        return None
    return {
        "pattern": "P2_ìˆ˜ê¸‰ë°˜ì „", "strength": min(strength, 3.0),
        "reasons": reasons, "desc": " / ".join(reasons),
    }


def detect_bb_squeeze(df: pd.DataFrame) -> dict | None:
    """P3: BB Squeeze â†’ í­ë°œ"""
    if "bb_width" not in df.columns or len(df) < 30:
        return None
    bw = df["bb_width"].dropna()
    if len(bw) < 30:
        return None
    recent_20 = bw.iloc[-20:]
    min_bw, curr_bw = float(recent_20.min()), float(bw.iloc[-1])
    prev_bw = float(bw.iloc[-2]) if len(bw) >= 2 else curr_bw
    avg_bw = float(recent_20.mean())
    if np.isnan(min_bw) or np.isnan(curr_bw):
        return None
    was_squeezed = min_bw < avg_bw * 0.7
    is_expanding = curr_bw > min_bw * 1.5 and curr_bw > prev_bw * 1.1
    if not (was_squeezed and is_expanding):
        return None
    vsr = float(df.iloc[-1].get("volume_surge_ratio", 1.0))
    vol_confirm = vsr > 1.5
    expansion = curr_bw / min_bw if min_bw > 0 else 1.0
    strength = min(expansion / 2.0, 2.5) + (0.5 if vol_confirm else 0)
    return {
        "pattern": "P3_BBìŠ¤í€´ì¦ˆ", "strength": min(strength, 3.0),
        "expansion": _sf(expansion), "vol_confirm": vol_confirm,
        "desc": f"BB ìˆ˜ì¶•â†’í­ë°œ ({expansion:.1f}ë°°)" + (" +ê±°ë˜ëŸ‰" if vol_confirm else ""),
    }


def detect_obv_divergence(df: pd.DataFrame) -> dict | None:
    """P4: OBV ë‹¤ì´ë²„ì „ìŠ¤ â€” ì€ë°€ ë§¤ì§‘"""
    if "obv" not in df.columns or len(df) < 20:
        return None
    tail = df.tail(20)
    close, obv = tail["close"].values, tail["obv"].values
    if np.any(np.isnan(close[-5:])) or np.any(np.isnan(obv[-5:])):
        return None
    price_chg_10 = (close[-1] / close[-10] - 1) * 100 if close[-10] > 0 else 0
    obv_start = obv[-10]
    if obv_start == 0:
        return None
    obv_chg_pct = (obv[-1] / obv_start - 1) * 100
    if not (-5 < price_chg_10 < 3 and obv_chg_pct > 3):
        return None
    vsr = float(df.iloc[-1].get("volume_surge_ratio", 1.0))
    vol_contracted = vsr < 0.7
    strength = min(obv_chg_pct / 10.0, 2.5) + (0.3 if vol_contracted else 0)
    return {
        "pattern": "P4_OBVë‹¤ì´ë²„ì „ìŠ¤", "strength": min(strength, 3.0),
        "price_chg_10d": _sf(price_chg_10), "obv_chg_10d": _sf(obv_chg_pct),
        "vol_contracted": vol_contracted,
        "desc": f"ê°€ê²© {price_chg_10:+.1f}% vs OBV +{obv_chg_pct:.1f}% (ì€ë°€ë§¤ì§‘"
                + (" +ì €ê±°ë˜ëŸ‰)" if vol_contracted else ")"),
    }


def detect_flow_exit(df: pd.DataFrame, csv_flow: dict) -> dict | None:
    """P5: ìˆ˜ê¸‰ ì´íƒˆ ê²½ê³ """
    if not csv_flow:
        if "ì™¸êµ­ì¸í•©ê³„" not in df.columns:
            return None
        tail = df.tail(5)
        if len(tail) < 5:
            return None
        f_5d = float(np.nansum(tail["ì™¸êµ­ì¸í•©ê³„"].values))
        i_5d = float(np.nansum(tail["ê¸°ê´€í•©ê³„"].values)) if "ê¸°ê´€í•©ê³„" in tail.columns else 0
        f_today = float(tail["ì™¸êµ­ì¸í•©ê³„"].iloc[-1]) if not np.isnan(tail["ì™¸êµ­ì¸í•©ê³„"].iloc[-1]) else 0
        i_today = float(tail["ê¸°ê´€í•©ê³„"].iloc[-1]) if "ê¸°ê´€í•©ê³„" in tail.columns and not np.isnan(tail["ê¸°ê´€í•©ê³„"].iloc[-1]) else 0
    else:
        f_5d, i_5d = csv_flow["f_last5"], csv_flow["i_last5"]
        f_today, i_today = csv_flow["f_today"], csv_flow["i_today"]

    if not (f_5d < 0 and i_5d < 0 and f_today < 0 and i_today < 0):
        return None
    total = abs(f_5d) + abs(i_5d)
    strength = min(1.0 + (total / 1e9), 3.0)
    return {
        "pattern": "P5_ìˆ˜ê¸‰ì´íƒˆ", "strength": min(strength, 3.0),
        "f_5d": _sf(f_5d), "i_5d": _sf(i_5d),
        "desc": f"ì™¸ì¸({f_5d:+,.0f})+ê¸°ê´€({i_5d:+,.0f}) ë™ì‹œë§¤ë„ 5ì¼ ì§€ì†",
    }


def detect_consecutive_accumulation(csv_flow: dict) -> dict | None:
    """P6: ì—°ì† ë§¤ì§‘ â€” ì™¸ì¸ ë˜ëŠ” ê¸°ê´€ 5ì¼+ ì—°ì† ìˆœë§¤ìˆ˜"""
    if not csv_flow:
        return None
    f_streak = csv_flow.get("f_streak", 0)
    i_streak = csv_flow.get("i_streak", 0)

    if f_streak < 5 and i_streak < 5:
        return None

    reasons = []
    strength = 0
    if f_streak >= 5:
        reasons.append(f"ì™¸ì¸ {f_streak}ì¼ ì—°ì† ìˆœë§¤ìˆ˜")
        strength += min(f_streak / 5.0, 2.0)
    if i_streak >= 5:
        reasons.append(f"ê¸°ê´€ {i_streak}ì¼ ì—°ì† ìˆœë§¤ìˆ˜")
        strength += min(i_streak / 5.0, 2.0)

    return {
        "pattern": "P6_ì—°ì†ë§¤ì§‘", "strength": min(strength, 3.0),
        "f_streak": f_streak, "i_streak": i_streak,
        "desc": " / ".join(reasons),
    }


def detect_vwap_breakout(df: pd.DataFrame) -> dict | None:
    """P7: VWAP ì´íƒˆ â€” ì¢…ê°€ê°€ ì¶”ì • VWAP ëŒ€ë¹„ í¬ê²Œ ê´´ë¦¬"""
    if len(df) < 20:
        return None
    # ê°„ì´ VWAP ì¶”ì •: ìµœê·¼ 20ì¼ (close * volume í•©) / volume í•©
    tail = df.tail(20)
    close_arr = tail["close"].values
    vol_arr = tail["volume"].values
    if np.any(np.isnan(close_arr)) or np.any(np.isnan(vol_arr)):
        return None
    total_vol = np.nansum(vol_arr)
    if total_vol <= 0:
        return None
    vwap_20 = float(np.nansum(close_arr * vol_arr) / total_vol)
    last_close = float(close_arr[-1])
    if vwap_20 <= 0:
        return None

    gap_pct = (last_close / vwap_20 - 1) * 100

    # ìƒë°© ì´íƒˆ (ë§¤ì§‘ í›„ ëŒíŒŒ) ë˜ëŠ” í•˜ë°© ì´íƒˆ (íˆ¬ë§¤)
    if abs(gap_pct) < 3:
        return None

    if gap_pct > 0:
        direction = "ìƒë°©ëŒíŒŒ"
        strength = min(gap_pct / 5.0, 2.5)
    else:
        direction = "í•˜ë°©ì´íƒˆ"
        strength = min(abs(gap_pct) / 5.0, 2.5)

    return {
        "pattern": "P7_VWAPì´íƒˆ", "strength": min(strength, 3.0),
        "vwap_20": round(vwap_20), "gap_pct": _sf(gap_pct),
        "direction": direction,
        "desc": f"VWAP20 ëŒ€ë¹„ {gap_pct:+.1f}% ({direction})",
    }


def classify_whale(patterns: list[dict]) -> str:
    """íŒ¨í„´ ì¡°í•©ìœ¼ë¡œ í•µì‹¬í•„í„° ë“±ê¸‰ ë¶„ë¥˜"""
    p_names = [p["pattern"] for p in patterns]
    has_exit = "P5_ìˆ˜ê¸‰ì´íƒˆ" in p_names
    positive_patterns = [p for p in patterns if p["pattern"] != "P5_ìˆ˜ê¸‰ì´íƒˆ"]

    if has_exit and not positive_patterns:
        return "ì´íƒˆê²½ê³ "
    obv_p = next((p for p in patterns if p["pattern"] == "P4_OBVë‹¤ì´ë²„ì „ìŠ¤"), None)
    if obv_p and obv_p.get("vol_contracted"):
        return "ë§¤ì§‘ì˜ì‹¬"
    # P6 ì—°ì†ë§¤ì§‘ í¬í•¨ ì‹œ ë§¤ì§‘ì˜ì‹¬
    if "P6_ì—°ì†ë§¤ì§‘" in p_names:
        if len(positive_patterns) >= 2:
            return "ì„¸ë ¥í¬ì°©"
        return "ë§¤ì§‘ì˜ì‹¬"
    if len(positive_patterns) >= 2:
        return "ì„¸ë ¥í¬ì°©"
    if len(positive_patterns) == 1:
        return "ì´ìƒê°ì§€"
    if has_exit and positive_patterns:
        return "í˜¼í•©ì‹œê·¸ë„"
    return "ì´ìƒê°ì§€"


def scan_anomaly() -> list[dict]:
    """Layer 2: ì „ì¢…ëª© ì´ìƒê±°ë˜ ìŠ¤ìº” (7íŒ¨í„´)"""
    name_map = build_name_map()
    parquets = sorted(PROCESSED_DIR.glob("*.parquet"))
    print(f"[Layer2] {len(parquets)}ê°œ ì¢…ëª© ì´ìƒê±°ë˜ ìŠ¤ìº”...")

    results = []
    for pq in parquets:
        ticker = pq.stem
        try:
            df = pd.read_parquet(pq)
            if len(df) < 30:
                continue
            df = df.tail(60)
            last = df.iloc[-1]

            close = float(last.get("close", 0))
            vol = float(last.get("volume", 0))
            if close < 1000 or vol < 1000 or close * vol < 5e8:
                continue

            csv_flow = get_flow_from_csv(ticker)

            detected = []
            for fn in [
                lambda: detect_volume_explosion(df),
                lambda: detect_flow_reversal(df, csv_flow),
                lambda: detect_bb_squeeze(df),
                lambda: detect_obv_divergence(df),
                lambda: detect_flow_exit(df, csv_flow),
                lambda: detect_consecutive_accumulation(csv_flow),
                lambda: detect_vwap_breakout(df),
            ]:
                p = fn()
                if p:
                    detected.append(p)

            if not detected:
                continue

            grade = classify_whale(detected)
            total_strength = sum(p["strength"] for p in detected)
            name = name_map.get(ticker, ticker)

            results.append({
                "ticker": ticker,
                "name": name,
                "close": int(close),
                "price_change": _sf(last.get("price_change", 0)),
                "volume": int(vol),
                "rsi": _sf(last.get("rsi_14", 50)),
                "volume_surge_ratio": _sf(last.get("volume_surge_ratio", 1)),
                "above_ma20": close > float(last.get("sma_20", 0)) if last.get("sma_20", 0) else False,
                "above_ma60": close > float(last.get("sma_60", 0)) if last.get("sma_60", 0) else False,
                "foreign_5d": _sf(last.get("foreign_net_5d", 0)),
                "grade": grade,
                "strength": _sf(total_strength),
                "pattern_count": len(detected),
                "patterns": detected,
            })
        except Exception as e:
            logger.debug("ì¢…ëª© %s ì²˜ë¦¬ ì‹¤íŒ¨: %s", ticker, e)

    grade_order = {"ì„¸ë ¥í¬ì°©": 0, "ë§¤ì§‘ì˜ì‹¬": 1, "ì´ìƒê°ì§€": 2, "í˜¼í•©ì‹œê·¸ë„": 3, "ì´íƒˆê²½ê³ ": 4}
    results.sort(key=lambda x: (grade_order.get(x["grade"], 9), -x["strength"]))
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Layer 3: ì´ë²¤íŠ¸ ë ˆì´ë” (ë‰´ìŠ¤ ê¸°ë°˜)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def scan_event_radar(surging_stocks: list[dict] | None = None) -> dict:
    """Layer 3: ë‰´ìŠ¤ ê¸°ë°˜ ì´ë²¤íŠ¸ ë ˆì´ë”

    Args:
        surging_stocks: Layer 2ì—ì„œ ê°ì§€ëœ ê¸‰ë“±ì£¼ ë¦¬ìŠ¤íŠ¸
            [{name, ticker, price_change, ...}, ...]
    """
    events = []
    theme_hits = []
    surging_news = []  # ê¸‰ë“±ì£¼ ìë™ ë‰´ìŠ¤

    # 1) market_news.jsonì—ì„œ high/medium ì„íŒ©íŠ¸ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    if MARKET_NEWS_PATH.exists():
        try:
            with open(MARKET_NEWS_PATH, "r", encoding="utf-8") as f:
                news_data = json.load(f)
            articles = news_data.get("articles", [])
            today = datetime.now().strftime("%Y-%m-%d")
            yesterday = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")

            for art in articles:
                if art.get("impact") in ("high", "medium"):
                    # ìµœê·¼ 2ì¼ ë‰´ìŠ¤ë§Œ
                    if art.get("date", "") >= yesterday:
                        events.append({
                            "title": art["title"],
                            "source": art.get("source", ""),
                            "date": art.get("date", ""),
                            "impact": art["impact"],
                            "url": art.get("url", ""),
                        })
        except Exception as e:
            logger.warning("market_news ë¡œë“œ ì‹¤íŒ¨: %s", e)

    # 2) theme_dictionary í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ â†’ ìˆ˜í˜œì¢…ëª© ë§¤ì¹­
    theme_dict = _load_theme_dict()
    all_news_titles = [e["title"] for e in events]

    if theme_dict and events:
        for event in events:
            title = event["title"]
            for theme_name, theme_data in theme_dict.items():
                keywords = theme_data.get("keywords", [])
                matched_kw = None
                for kw in keywords:
                    if kw.lower() in title.lower():
                        matched_kw = kw
                        break
                if matched_kw:
                    stocks = theme_data.get("stocks", [])
                    theme_hits.append({
                        "theme": theme_name,
                        "keyword": matched_kw,
                        "news_title": title,
                        "news_date": event.get("date", ""),
                        "impact": event["impact"],
                        "stocks": stocks[:5],  # ìƒìœ„ 5ì¢…ëª©
                    })

    # 3) DART ê³µì‹œ ì—°ë™ (ë‰´ìŠ¤ ëŒ€ë¹„ 30ë¶„~ìˆ˜ì‹œê°„ ì„ í–‰)
    dart_disclosures = []
    if DART_PATH.exists():
        try:
            with open(DART_PATH, "r", encoding="utf-8") as f:
                dart_data = json.load(f)
            # tier1 + tier2 ê³µì‹œë¥¼ ì´ë²¤íŠ¸ë¡œ ì¶”ê°€
            for tier_key in ("tier1", "tier2"):
                for d in dart_data.get(tier_key, []):
                    dart_disclosures.append({
                        "title": f"[ê³µì‹œ] {d['corp_name']} â€” {d['report_nm'][:50]}",
                        "source": "DART",
                        "date": d.get("rcept_dt", "")[:4] + "-" + d.get("rcept_dt", "")[4:6] + "-" + d.get("rcept_dt", "")[6:8] if len(d.get("rcept_dt", "")) == 8 else "",
                        "impact": "high" if "tier1" in d.get("tier", "") else "medium",
                        "tier": d.get("tier", ""),
                        "keyword": d.get("keyword", ""),
                        "corp_name": d.get("corp_name", ""),
                        "stock_code": d.get("stock_code", ""),
                        "url": d.get("url", ""),
                    })
            # ìœ ë‹ˆë²„ìŠ¤ ê´€ë ¨ ê³µì‹œ ë³„ë„ ìˆ˜ì§‘
            universe_dart = dart_data.get("universe_hits", [])
            logger.info("  DART ê³µì‹œ: tier1+2 %dê±´, ìœ ë‹ˆë²„ìŠ¤ ê´€ë ¨ %dê±´",
                        len(dart_disclosures), len(universe_dart))
        except Exception as e:
            logger.warning("DART ê³µì‹œ ë¡œë“œ ì‹¤íŒ¨: %s", e)
            universe_dart = []
    else:
        universe_dart = []

    # DART ê³µì‹œë„ í…Œë§ˆ ë§¤ì¹­
    if theme_dict and dart_disclosures:
        for disc in dart_disclosures:
            title = disc["title"]
            for theme_name, theme_data in theme_dict.items():
                for kw in theme_data.get("keywords", []):
                    if kw.lower() in title.lower():
                        theme_hits.append({
                            "theme": theme_name,
                            "keyword": kw,
                            "news_title": title,
                            "news_date": disc.get("date", ""),
                            "impact": disc["impact"],
                            "stocks": theme_data.get("stocks", [])[:5],
                            "source": "DARTê³µì‹œ",
                        })
                        break

    # 4) ê¸‰ë“±ì£¼ ìë™ ë‰´ìŠ¤ ê²€ìƒ‰ (P1 ê±°ë˜ëŸ‰í­ë°œ + ê°€ê²©ë³€ë™ >=8%)
    if surging_stocks:
        stock_names = [s["name"] for s in surging_stocks[:10]]  # ìµœëŒ€ 10ì¢…ëª©
        try:
            from scripts.crawl_market_news import crawl_stock_news
            raw_news = crawl_stock_news(stock_names, days=3)
            for n in raw_news:
                surging_news.append({
                    "stock_name": n["stock_name"],
                    "title": n["title"],
                    "source": n.get("source", ""),
                    "date": n.get("date", ""),
                    "impact": n.get("impact", "medium"),
                    "url": n.get("url", ""),
                })
                # ê¸‰ë“±ì£¼ ë‰´ìŠ¤ë„ í…Œë§ˆ ë§¤ì¹­ ì‹œë„
                if theme_dict:
                    for theme_name, theme_data in theme_dict.items():
                        for kw in theme_data.get("keywords", []):
                            if kw.lower() in n["title"].lower():
                                theme_hits.append({
                                    "theme": theme_name,
                                    "keyword": kw,
                                    "news_title": n["title"],
                                    "news_date": n.get("date", ""),
                                    "impact": "high",
                                    "stocks": [{"name": n["stock_name"]}],
                                    "source": "ê¸‰ë“±ì£¼ë‰´ìŠ¤",
                                })
                                break
            logger.info(f"  ê¸‰ë“±ì£¼ ë‰´ìŠ¤: {len(surging_news)}ê±´ ({len(stock_names)}ì¢…ëª©)")
        except Exception as e:
            logger.warning("ê¸‰ë“±ì£¼ ë‰´ìŠ¤ í¬ë¡¤ ì‹¤íŒ¨: %s", e)

    # ì´ë²¤íŠ¸ ìš”ì•½ í†µê³„
    high_count = sum(1 for e in events if e["impact"] == "high")
    med_count = sum(1 for e in events if e["impact"] == "medium")

    # ì‹œì¥ ë¶„ìœ„ê¸° íŒì •
    if high_count >= 3:
        mood = "ê¸´ì¥"
        mood_desc = f"HIGH ì„íŒ©íŠ¸ ë‰´ìŠ¤ {high_count}ê±´ â€” ë³€ë™ì„± ì£¼ì˜"
    elif high_count >= 1:
        mood = "ê²½ê³„"
        mood_desc = f"HIGH {high_count}ê±´ + MED {med_count}ê±´ â€” ì£¼ìš” ì´ìŠˆ ëª¨ë‹ˆí„°ë§"
    elif med_count >= 3:
        mood = "ê´€ì‹¬"
        mood_desc = f"ì¤‘ìš” ë‰´ìŠ¤ {med_count}ê±´ â€” í…Œë§ˆ/ì´ë²¤íŠ¸ ì£¼ì‹œ"
    elif surging_news:
        mood = "ê´€ì‹¬"
        mood_desc = f"ê¸‰ë“±ì£¼ {len(surging_stocks)}ê°œ ë‰´ìŠ¤ {len(surging_news)}ê±´ â€” ì´ë²¤íŠ¸ í™•ì¸"
    else:
        mood = "í‰ì˜¨"
        mood_desc = "íŠ¹ë³„í•œ ì´ë²¤íŠ¸ ì—†ìŒ â€” ê¸°ìˆ ì  ë¶„ì„ ìš°ì„ "

    # theme_hits ì¤‘ë³µ ì œê±°
    seen_theme = set()
    unique_themes = []
    for th in theme_hits:
        key = (th["theme"], th["news_title"][:30])
        if key not in seen_theme:
            seen_theme.add(key)
            unique_themes.append(th)

    return {
        "mood": mood,
        "mood_desc": mood_desc,
        "event_count": len(events),
        "high_impact": high_count,
        "medium_impact": med_count,
        "events": events[:15],  # ìµœëŒ€ 15ê±´
        "theme_hits": unique_themes[:15],  # ìµœëŒ€ 15ê±´
        "surging_stock_news": surging_news[:20],  # ê¸‰ë“±ì£¼ ë‰´ìŠ¤ ìµœëŒ€ 20ê±´
        "dart_disclosures": dart_disclosures[:20],  # DART ê³µì‹œ ìµœëŒ€ 20ê±´
        "dart_universe": universe_dart[:15],  # ìœ ë‹ˆë²„ìŠ¤ ê´€ë ¨ DART ê³µì‹œ
    }


def _load_theme_dict() -> dict:
    """theme_dictionary.yaml ë¡œë“œ"""
    if not THEME_DICT_PATH.exists():
        return {}
    try:
        import yaml
        with open(THEME_DICT_PATH, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
        return data.get("themes", {})
    except Exception:
        return {}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸ í†µí•©
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    parser = argparse.ArgumentParser(description="ì„¸ë ¥ê°ì§€ í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ìºë„ˆ")
    parser.add_argument("--top", type=int, default=30, help="ì´ìƒê±°ë˜ ì¶œë ¥ ì¢…ëª© ìˆ˜")
    args = parser.parse_args()

    logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

    print("=" * 60)
    print("  ì„¸ë ¥ê°ì§€ í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ìºë„ˆ (3-Layer)")
    print("=" * 60)

    # Layer 1: ìˆ˜ê¸‰ê±´ì „ì„±
    print("\n[Layer 1] ìˆ˜ê¸‰ê±´ì „ì„± ë¶„ì„ ì¤‘...")
    health = analyze_supply_demand_health()
    alert = health["alert"]
    print(f"  â†’ ì¢…í•© ê²½ë³´: {alert} (ì ìˆ˜ {health['score']})")
    print(f"  â†’ ì™¸ì¸ ìˆœë§¤ìˆ˜ ë¹„ìœ¨: {health['foreign']['buying_ratio']}%")
    print(f"  â†’ ê¸°ê´€ ìˆœë§¤ìˆ˜ ë¹„ìœ¨: {health['institution']['buying_ratio']}%")

    # Layer 2: ì´ìƒê±°ë˜
    anomaly_items = scan_anomaly()
    anomaly_stats = {}
    for r in anomaly_items:
        g = r["grade"]
        anomaly_stats[g] = anomaly_stats.get(g, 0) + 1

    print(f"\n[Layer2] ì´ {len(anomaly_items)}ê±´ ì´ìƒê±°ë˜ íƒì§€")
    for g in ["ì„¸ë ¥í¬ì°©", "ë§¤ì§‘ì˜ì‹¬", "ì´ìƒê°ì§€", "í˜¼í•©ì‹œê·¸ë„", "ì´íƒˆê²½ê³ "]:
        cnt = anomaly_stats.get(g, 0)
        if cnt:
            print(f"  {g}: {cnt}ê±´")

    # Layer 2 â†’ Layer 3 ì—°ê³„: P1 ê±°ë˜ëŸ‰í­ë°œ + ê°€ê²©ë³€ë™ >=8% â†’ ê¸‰ë“±ì£¼
    surging_stocks = []
    for item in anomaly_items:
        has_p1 = any(p["pattern"] == "P1_ê±°ë˜ëŸ‰í­ë°œ" for p in item["patterns"])
        pct = abs(item.get("price_change", 0))
        if has_p1 and pct >= 8:
            surging_stocks.append(item)
    if surging_stocks:
        print(f"\n[Layer2â†’3] ê¸‰ë“±ì£¼ {len(surging_stocks)}ê°œ ë‰´ìŠ¤ ìë™ê²€ìƒ‰ ëŒ€ìƒ:")
        for s in surging_stocks[:5]:
            print(f"  â†’ {s['name']}({s['ticker']}) {s['price_change']:+.1f}%")

    # Layer 3: ì´ë²¤íŠ¸ ë ˆì´ë”
    print("\n[Layer 3] ì´ë²¤íŠ¸ ë ˆì´ë” ìŠ¤ìº” ì¤‘...")
    radar = scan_event_radar(surging_stocks=surging_stocks if surging_stocks else None)
    print(f"  â†’ ì‹œì¥ ë¶„ìœ„ê¸°: {radar['mood']} ({radar['mood_desc']})")
    print(f"  â†’ ì´ë²¤íŠ¸ {radar['event_count']}ê±´ (HIGH:{radar['high_impact']} MED:{radar['medium_impact']})")
    if radar["theme_hits"]:
        print(f"  â†’ í…Œë§ˆ ë§¤ì¹­: {len(radar['theme_hits'])}ê±´")
        for th in radar["theme_hits"][:5]:
            print(f"    [{th['theme']}] {th['keyword']} â€” {th['news_title'][:40]}...")
    if radar.get("surging_stock_news"):
        print(f"  â†’ ê¸‰ë“±ì£¼ ë‰´ìŠ¤: {len(radar['surging_stock_news'])}ê±´")
        for sn in radar["surging_stock_news"][:5]:
            print(f"    [{sn['stock_name']}] {sn['title'][:50]}...")
    if radar.get("dart_disclosures"):
        print(f"  â†’ DART ê³µì‹œ: {len(radar['dart_disclosures'])}ê±´ (tier1+2)")
        for dc in radar["dart_disclosures"][:5]:
            icon = "ğŸ”´" if dc.get("impact") == "high" else "ğŸŸ¡"
            print(f"    {icon} {dc['corp_name']}({dc.get('stock_code','')}) [{dc.get('keyword','')}]")
    if radar.get("dart_universe"):
        print(f"  â†’ ìœ ë‹ˆë²„ìŠ¤ DART: {len(radar['dart_universe'])}ê±´")
        for du in radar["dart_universe"][:5]:
            print(f"    ğŸ¯ {du['corp_name']}({du['stock_code']}) â€” {du['report_nm'][:40]}")

    # í¬ë¡œìŠ¤ ë¶„ì„: ìˆ˜ê¸‰ê±´ì „ì„± Ã— ì´ìƒê±°ë˜ ë§¥ë½ í•´ì„
    cross_insights = []
    if health["alert"] == "ìœ„í—˜":
        exit_count = anomaly_stats.get("ì´íƒˆê²½ê³ ", 0)
        if exit_count > 0:
            cross_insights.append(f"ìˆ˜ê¸‰ ìœ„í—˜ + ì´íƒˆê²½ê³  {exit_count}ê±´ â†’ ì‹œì¥ ë¦¬ìŠ¤í¬ ìƒìŠ¹")
        accum_count = anomaly_stats.get("ë§¤ì§‘ì˜ì‹¬", 0) + anomaly_stats.get("ì„¸ë ¥í¬ì°©", 0)
        if accum_count > 0:
            cross_insights.append(f"ìˆ˜ê¸‰ ìœ„í—˜ ì† ë§¤ì§‘ {accum_count}ê±´ â†’ ì—­ë°œìƒ ë§¤ìˆ˜ í›„ë³´ (ì£¼ì˜)")
    elif health["alert"] == "ì•ˆì „":
        accum_count = anomaly_stats.get("ë§¤ì§‘ì˜ì‹¬", 0) + anomaly_stats.get("ì„¸ë ¥í¬ì°©", 0)
        if accum_count > 0:
            cross_insights.append(f"ìˆ˜ê¸‰ ì•ˆì „ + ë§¤ì§‘ {accum_count}ê±´ â†’ ê°•í•œ ë§¤ìˆ˜ ì‹ í˜¸")

    if cross_insights:
        print(f"\n[í¬ë¡œìŠ¤ ë¶„ì„]")
        for insight in cross_insights:
            print(f"  â†’ {insight}")

    # ìƒìœ„ ì¢…ëª© ì¶œë ¥
    print(f"\n{'â”€'*60}")
    print(f"[ì´ìƒê±°ë˜ TOP {min(args.top, len(anomaly_items))}]")
    for i, r in enumerate(anomaly_items[:args.top], 1):
        pats = " + ".join(p["pattern"].split("_")[1] for p in r["patterns"])
        print(f"  {i:2d}. [{r['grade']}] {r['name']}({r['ticker']}) "
              f"ì¢…ê°€ {r['close']:,} ({r['price_change']:+.1f}%) "
              f"ê°•ë„ {r['strength']:.1f} â€” {pats}")

    # JSON ì €ì¥
    output = {
        "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M"),
        "supply_demand_health": health,
        "anomaly": {
            "total_detected": len(anomaly_items),
            "stats": anomaly_stats,
            "items": anomaly_items[:args.top],
        },
        "event_radar": radar,
        "cross_insights": cross_insights,
    }
    OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    print(f"\n[ì €ì¥] {OUTPUT_PATH}")


if __name__ == "__main__":
    main()
