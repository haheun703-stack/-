"""
ì„¸ë ¥ ë§¤ì§‘ ì¶”ì ê¸° â€” Volume Explosion â†’ Multi-Phase Rally Detector

ì°¨íŠ¸ íŒ¨í„´ ì‹¤ì¦:
  ê±°ë˜ëŸ‰ í­ë°œ â†’ ì ì§„ì  ìƒìŠ¹ â†’ ì¡°ì • â†’ ê¸‰ìƒìŠ¹ â†’ ë°˜ë³µ

ì ‘ê·¼ë²•:
  1. ê³¼ê±° 60ì¼ê°„ ëª¨ë“  parquetì—ì„œ vol_z >= 3.0 ë˜ëŠ” VSR >= 3.0 ë°œìƒ ì´ë ¥ íƒìƒ‰
  2. í­ë°œ ì´í›„ OBV/TRIX/ê¸°ê´€ë§¤ìˆ˜/ê°€ê²©êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ë§¤ì§‘ ì§„í–‰ ì—¬ë¶€ íŒì •
  3. í˜ì´ì¦ˆ ë¶„ë¥˜:
     Phase 1 (í­ë°œ): í­ë°œì¼ 0~5ì¼ ì´ë‚´
     Phase 2 (ë§¤ì§‘): 5~25ì¼, ê±°ë˜ëŸ‰â†“ + OBVâ†‘ + ê°€ê²© íš¡ë³´/ìƒìŠ¹
     Phase 3 (ì¬ëŒíŒŒ): TRIX 0ì„  ëŒíŒŒ + MACD ìƒìŠ¹ + ê±°ë˜ëŸ‰ ì¬ì¦ê°€ ì¡°ì§
     Phase 4 (ê°€ì†): 2ì°¨ ê±°ë˜ëŸ‰ í­ë°œ + íŒŒë¼ë³¼ë¦­ ìƒìŠ¹
  4. ë§¤ì§‘ ì ìˆ˜ (0~100) ê¸°ë°˜ ì¶”ì²œ

ì¶œë ¥: data/accumulation_tracker.json
BAT-D 12.7ë‹¨ê³„ (ìˆ˜ê¸‰í­ë°œ ì´í›„)

Usage:
    python scripts/scan_accumulation_tracker.py
"""

from __future__ import annotations

import json
import logging
import sys
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd

PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

logger = logging.getLogger(__name__)

DATA_DIR = PROJECT_ROOT / "data"
PROCESSED_DIR = DATA_DIR / "processed"
CSV_DIR = PROJECT_ROOT / "stock_data_daily"
OUTPUT_PATH = DATA_DIR / "accumulation_tracker.json"

# â”€â”€ íŒŒë¼ë¯¸í„° â”€â”€
SPIKE_VOL_Z = 3.0           # í­ë°œ ì„ê³„ì¹˜ (vol_z)
SPIKE_VSR = 3.0             # í­ë°œ ì„ê³„ì¹˜ (volume_surge_ratio)
LOOKBACK_DAYS = 60          # ê³¼ê±° ëª‡ì¼ê¹Œì§€ í­ë°œ ì´ë ¥ íƒìƒ‰
MIN_SPIKE_AGE = 3           # í­ë°œ í›„ ìµœì†Œ ê²½ê³¼ì¼ (ë„ˆë¬´ ì´ë¥¸ê±´ Phase1)
MIN_SCORE = 40              # ìµœì†Œ ì ìˆ˜ (ì´í•˜ëŠ” ì¶œë ¥ ì•ˆí•¨)


def build_name_map() -> dict[str, str]:
    name_map = {}
    for csv in CSV_DIR.glob("*.csv"):
        parts = csv.stem.rsplit("_", 1)
        if len(parts) == 2:
            name_map[parts[1]] = parts[0]
    return name_map


def find_historical_spikes(df: pd.DataFrame, lookback: int = LOOKBACK_DAYS) -> list[dict]:
    """ê³¼ê±° lookbackì¼ ë‚´ ê±°ë˜ëŸ‰ í­ë°œ ì´ë ¥ íƒìƒ‰.

    Returns: [{idx, date, close, high, vol_z, vsr, volume}, ...]
    """
    if len(df) < lookback + 5:
        lookback = max(len(df) - 5, 10)

    recent = df.tail(lookback)
    spikes = []

    for i, (date_idx, row) in enumerate(recent.iterrows()):
        vol_z = float(row.get("vol_z", 0) or 0)
        vsr = float(row.get("volume_surge_ratio", 0) or 0)
        if pd.isna(vol_z):
            vol_z = 0
        if pd.isna(vsr):
            vsr = 0

        if vol_z >= SPIKE_VOL_Z or vsr >= SPIKE_VSR:
            spikes.append({
                "row_idx": len(df) - len(recent) + i,
                "date": str(date_idx)[:10] if hasattr(date_idx, "strftime") else str(date_idx)[:10],
                "close": float(row.get("close", 0)),
                "high": float(row.get("high", 0)),
                "vol_z": round(vol_z, 2),
                "vsr": round(vsr, 2),
                "volume": int(row.get("volume", 0)),
            })

    return spikes


def analyze_post_spike(df: pd.DataFrame, spike: dict) -> dict | None:
    """í­ë°œ ì´í›„ ë§¤ì§‘ ì§„í–‰ ìƒí™© ë¶„ì„.

    Returns: ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None (ë¶„ì„ ë¶ˆê°€)
    """
    spike_idx = spike["row_idx"]
    total_rows = len(df)
    days_since = total_rows - 1 - spike_idx  # í­ë°œ í›„ ê²½ê³¼ ê±°ë˜ì¼

    if days_since < MIN_SPIKE_AGE:
        return None  # ë„ˆë¬´ ì´ë¥´ë©´ ìŠ¤í‚µ

    # í­ë°œ ì´í›„ ë°ì´í„°
    post_df = df.iloc[spike_idx:]
    if len(post_df) < 3:
        return None

    last = df.iloc[-1]
    spike_close = spike["close"]
    spike_high = spike["high"]
    current_close = float(last.get("close", 0))

    if spike_close <= 0 or current_close <= 0:
        return None

    # í­ë°œ ì´í›„ ìˆ˜ìµë¥ 
    return_since_spike = (current_close / spike_close - 1) * 100

    # -30% ì´ìƒ ê¸‰ë½í•œ ì¢…ëª©ì€ ë§¤ì§‘ ì‹¤íŒ¨ë¡œ íŒì •
    if return_since_spike < -30:
        return None

    # â”€â”€ 4ì¶• ë§¤ì§‘ ì ìˆ˜ ê³„ì‚° â”€â”€

    # ì¶•1: OBV ì¶”ì„¸ (25ì )
    obv_score = _score_obv_trend(df, spike_idx)

    # ì¶•2: ê¸°ê´€/ì™¸ì¸ ë§¤ì§‘ íŒ¨í„´ (25ì )
    flow_score = _score_institutional_flow(df, spike_idx)

    # ì¶•3: TRIX/MACD ëª¨ë©˜í…€ (25ì )
    momentum_score = _score_momentum(df, spike_idx)

    # ì¶•4: ê°€ê²© êµ¬ì¡° (25ì )
    structure_score = _score_price_structure(df, spike_idx)

    total = obv_score + flow_score + momentum_score + structure_score

    # í˜ì´ì¦ˆ ë¶„ë¥˜
    phase = _classify_phase(days_since, df, spike_idx)

    return {
        "days_since_spike": days_since,
        "spike_date": spike["date"],
        "spike_close": int(spike_close),
        "spike_high": int(spike_high),
        "spike_vol_z": spike["vol_z"],
        "current_close": int(current_close),
        "return_since_spike": round(return_since_spike, 1),
        "phase": phase,
        "total_score": round(total, 1),
        "obv_score": obv_score,
        "flow_score": flow_score,
        "momentum_score": momentum_score,
        "structure_score": structure_score,
    }


def _score_obv_trend(df: pd.DataFrame, spike_idx: int) -> int:
    """ì¶•1: OBV ì¶”ì„¸ ì ìˆ˜ (0~25).

    í•µì‹¬: í­ë°œ ì´í›„ OBVê°€ í•˜ë½í•˜ì§€ ì•Šìœ¼ë©´ ë§¤ì§‘ ì§„í–‰ ì¤‘.
    - OBVê°€ spike ì‹œì  ëŒ€ë¹„ ìƒìŠ¹: 20~25ì 
    - OBV íš¡ë³´(Â±5%): 10~15ì 
    - OBV í•˜ë½: 0~5ì 
    """
    if "obv" not in df.columns:
        return 10  # ë°ì´í„° ì—†ìœ¼ë©´ ì¤‘ë¦½

    post_df = df.iloc[spike_idx:]
    if len(post_df) < 3:
        return 10

    spike_obv = float(post_df.iloc[0]["obv"])
    current_obv = float(post_df.iloc[-1]["obv"])

    if spike_obv == 0:
        return 10

    obv_change_pct = (current_obv / spike_obv - 1) * 100

    # OBV 5ì¼ ì¶”ì„¸ (ìµœê·¼)
    obv_5d_trend = float(df.iloc[-1].get("obv_trend_5d", 0) or 0)

    score = 0

    # OBV ì ˆëŒ€ ë³€í™”
    if obv_change_pct > 10:
        score += 18
    elif obv_change_pct > 5:
        score += 15
    elif obv_change_pct > 0:
        score += 12
    elif obv_change_pct > -5:
        score += 8
    else:
        score += 3

    # ìµœê·¼ 5ì¼ OBV ì¶”ì„¸ ë³´ë„ˆìŠ¤
    if obv_5d_trend > 0:
        score += 7
    elif obv_5d_trend == 0:
        score += 4
    else:
        score += 0

    return min(score, 25)


def _score_institutional_flow(df: pd.DataFrame, spike_idx: int) -> int:
    """ì¶•2: ê¸°ê´€/ì™¸ì¸ ë§¤ì§‘ íŒ¨í„´ (0~25).

    í•µì‹¬: í­ë°œ ì´í›„ ì™¸ì¸/ê¸°ê´€ì´ ê³„ì† ì‚¬ëŠ”ê°€?
    - ì™¸ì¸ ì—°ì†ë§¤ìˆ˜: ë†’ì€ ì ìˆ˜
    - ê¸°ê´€ 5ì¼ ìˆœë§¤ìˆ˜: ë³´ë„ˆìŠ¤
    - ë™ì‹œë§¤ìˆ˜: ì¶”ê°€ ë³´ë„ˆìŠ¤
    """
    last = df.iloc[-1]
    score = 0

    # ì™¸ì¸ ì—°ì†ë§¤ìˆ˜ì¼
    foreign_consec = int(last.get("foreign_consecutive_buy", 0) or 0)
    if foreign_consec >= 5:
        score += 12
    elif foreign_consec >= 3:
        score += 8
    elif foreign_consec >= 1:
        score += 4

    # ì™¸ì¸ 5ì¼ ìˆœë§¤ìˆ˜
    foreign_5d = float(last.get("foreign_net_5d", 0) or 0)
    if pd.isna(foreign_5d):
        foreign_5d = 0
    if foreign_5d > 0:
        score += 5
    elif foreign_5d > -1e6:
        score += 2

    # ê¸°ê´€ 5ì¼ ìˆœë§¤ìˆ˜ (ìˆ˜ë™ ê³„ì‚°)
    if "ê¸°ê´€í•©ê³„" in df.columns:
        inst_5d = float(df["ê¸°ê´€í•©ê³„"].tail(5).sum())
        if pd.isna(inst_5d):
            inst_5d = 0
        if inst_5d > 0:
            score += 5
        elif inst_5d > -1e6:
            score += 2
    else:
        inst_5d = 0

    # ë™ì‹œë§¤ìˆ˜ ë³´ë„ˆìŠ¤
    if foreign_5d > 0 and inst_5d > 0:
        score += 3

    return min(score, 25)


def _score_momentum(df: pd.DataFrame, spike_idx: int) -> int:
    """ì¶•3: TRIX/MACD ëª¨ë©˜í…€ (0~25).

    í•µì‹¬: TRIX 0ì„  ëŒíŒŒ = ì¤‘ê¸° ìƒìŠ¹ í™•ì •, MACD í™•ì¥ = ë‹¨ê¸° ê°€ì†
    """
    last = df.iloc[-1]
    score = 0

    # TRIX 0ì„  ìœ„ì¹˜
    trix = float(last.get("trix", 0) or 0)
    trix_signal = float(last.get("trix_signal", 0) or 0)
    trix_gx = bool(last.get("trix_golden_cross", 0))

    if trix > 0 and trix > trix_signal:
        score += 12  # TRIX 0ì„  ìœ„ + ì‹œê·¸ë„ ìœ„ = ê°•ì„¸
    elif trix > 0:
        score += 8   # TRIX 0ì„  ìœ„
    elif trix > trix_signal:
        score += 5   # TRIX ìƒìŠ¹ ì¤‘ì´ì§€ë§Œ ì•„ì§ 0ì„  ì•„ë˜
    else:
        score += 0

    if trix_gx:
        score += 3  # ê³¨ë“ í¬ë¡œìŠ¤ ë³´ë„ˆìŠ¤

    # MACD íˆìŠ¤í† ê·¸ë¨ ë°©í–¥
    macd_hist = float(last.get("macd_histogram", 0) or 0)
    macd_hist_prev = float(last.get("macd_histogram_prev", 0) or 0)

    if macd_hist > 0 and macd_hist > macd_hist_prev:
        score += 7  # MACD ì–‘ìˆ˜ + í™•ì¥
    elif macd_hist > 0:
        score += 5  # MACD ì–‘ìˆ˜
    elif macd_hist > macd_hist_prev:
        score += 3  # MACD ê°œì„  ì¤‘
    else:
        score += 0

    # RSI ì ì •ëŒ€ ë³´ë„ˆìŠ¤ (40~65)
    rsi = float(last.get("rsi_14", 50) or 50)
    if 40 <= rsi <= 65:
        score += 3
    elif 35 <= rsi <= 70:
        score += 1

    return min(score, 25)


def _score_price_structure(df: pd.DataFrame, spike_idx: int) -> int:
    """ì¶•4: ê°€ê²© êµ¬ì¡° (0~25).

    í•µì‹¬: Higher Low + MA ì§€ì§€ + ì ì • ì¡°ì •í­
    """
    last = df.iloc[-1]
    post_df = df.iloc[spike_idx:]
    if len(post_df) < 3:
        return 10

    score = 0
    close = float(last.get("close", 0))
    ma20 = float(last.get("sma_20", 0) or 0)
    ma60 = float(last.get("sma_60", 0) or 0)

    # MA20 ìœ„
    if close > ma20 > 0:
        score += 5
    # MA60 ìœ„
    if close > ma60 > 0:
        score += 5

    # Higher Low êµ¬ì¡°: ìµœê·¼ 10ì¼ ìµœì €ê°€ > í­ë°œ í›„ 5ì¼ë‚´ ìµœì €ê°€
    spike_close = float(post_df.iloc[0]["close"])
    early_low = float(post_df.head(min(10, len(post_df)))["low"].min())
    recent_low = float(df.tail(10)["low"].min()) if len(df) >= 10 else early_low

    if recent_low > early_low:
        score += 5  # Higher Low
    elif recent_low >= early_low * 0.97:
        score += 3  # ìœ ì‚¬ ìˆ˜ì¤€ ìœ ì§€

    # í­ë°œ í›„ ìµœê³ ì  ëŒ€ë¹„ ì¡°ì •í­
    post_high = float(post_df["high"].max())
    if post_high > 0:
        drawdown_from_peak = (close / post_high - 1) * 100
        # -3~-15% ì¡°ì •ì´ ì´ìƒì  (ëˆŒë¦¼ëª©)
        if -15 <= drawdown_from_peak <= -3:
            score += 7  # ì ì • ì¡°ì • êµ¬ê°„
        elif -3 < drawdown_from_peak <= 0:
            score += 5  # ê³ ì  ê·¼ì²˜ (ê°•ì„¸)
        elif -20 <= drawdown_from_peak < -15:
            score += 3  # ì•½ê°„ ê¹Šì€ ì¡°ì •
        else:
            score += 0  # ê³¼ë„í•œ í•˜ë½

    # ì¶”ì„¸ì„  ê¸°ìš¸ê¸° (linreg_slope_20)
    slope = float(last.get("linreg_slope_20", 0) or 0)
    if slope > 0:
        score += 3

    return min(score, 25)


def _classify_phase(days_since: int, df: pd.DataFrame, spike_idx: int) -> str:
    """í­ë°œ ì´í›„ í˜„ì¬ í˜ì´ì¦ˆ ë¶„ë¥˜."""
    last = df.iloc[-1]
    trix = float(last.get("trix", 0) or 0)
    vol_z = float(last.get("vol_z", 0) or 0)
    if pd.isna(vol_z):
        vol_z = 0
    vsr = float(last.get("volume_surge_ratio", 0) or 0)
    if pd.isna(vsr):
        vsr = 0

    # Phase 4: 2ì°¨ ê±°ë˜ëŸ‰ í­ë°œ (ë‹¤ì‹œ vol_z >= 2.5)
    if days_since >= 10 and (vol_z >= 2.5 or vsr >= 2.5):
        return "ê°€ì†"

    # Phase 3: TRIX 0ì„  ëŒíŒŒ + 10ì¼ ì´ìƒ ê²½ê³¼
    if days_since >= 8 and trix > 0:
        return "ì¬ëŒíŒŒ"

    # Phase 2: ë§¤ì§‘ êµ¬ê°„
    if days_since >= MIN_SPIKE_AGE:
        return "ë§¤ì§‘"

    return "í­ë°œ"


def scan_all_stocks(name_map: dict) -> list[dict]:
    """ì „ì²´ parquet ìŠ¤ìº”í•˜ì—¬ ë§¤ì§‘ ì§„í–‰ ì¤‘ì¸ ì¢…ëª© íƒì§€."""
    results = []

    for pq in sorted(PROCESSED_DIR.glob("*.parquet")):
        ticker = pq.stem
        try:
            df = pd.read_parquet(pq)
            if len(df) < 30:
                continue

            # ê³¼ê±° í­ë°œ ì´ë ¥ íƒìƒ‰
            spikes = find_historical_spikes(df, LOOKBACK_DAYS)
            if not spikes:
                continue

            # ê°€ì¥ ìµœê·¼ í­ë°œë§Œ ì‚¬ìš© (ê°€ì¥ ê´€ë ¨ì„± ë†’ìŒ)
            latest_spike = spikes[-1]

            # í­ë°œ ì´í›„ ë§¤ì§‘ ë¶„ì„
            analysis = analyze_post_spike(df, latest_spike)
            if analysis is None:
                continue

            if analysis["total_score"] < MIN_SCORE:
                continue

            # ë©€í‹° ìŠ¤íŒŒì´í¬ ë³´ë„ˆìŠ¤ (60ì¼ ë‚´ 2íšŒ ì´ìƒ í­ë°œ = ì§€ì†ì  ê´€ì‹¬)
            multi_spike = len(spikes) >= 2
            if multi_spike:
                analysis["total_score"] = min(analysis["total_score"] + 5, 100)

            name = name_map.get(ticker, ticker)
            last = df.iloc[-1]

            results.append({
                "ticker": ticker,
                "name": name,
                "phase": analysis["phase"],
                "total_score": analysis["total_score"],
                "days_since_spike": analysis["days_since_spike"],
                "spike_date": analysis["spike_date"],
                "spike_close": analysis["spike_close"],
                "current_close": analysis["current_close"],
                "return_since_spike": analysis["return_since_spike"],
                "spike_vol_z": analysis["spike_vol_z"],
                "obv_score": analysis["obv_score"],
                "flow_score": analysis["flow_score"],
                "momentum_score": analysis["momentum_score"],
                "structure_score": analysis["structure_score"],
                "n_spikes_60d": len(spikes),
                "rsi": round(float(last.get("rsi_14", 50) or 50), 1),
                "trix": round(float(last.get("trix", 0) or 0), 3),
                "foreign_consec": int(last.get("foreign_consecutive_buy", 0) or 0),
            })

        except Exception as e:
            logger.debug("ë¶„ì„ ì‹¤íŒ¨ %s: %s", ticker, e)

    # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ
    results.sort(key=lambda x: -x["total_score"])
    return results


def main():
    logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

    name_map = build_name_map()

    print("[ë§¤ì§‘ ì¶”ì ê¸°] ì „ì²´ parquet ìŠ¤ìº” ì‹œì‘...")
    results = scan_all_stocks(name_map)

    # í˜ì´ì¦ˆë³„ í†µê³„
    phase_stats = {}
    for r in results:
        p = r["phase"]
        phase_stats[p] = phase_stats.get(p, 0) + 1

    print(f"\n{'='*60}")
    print(f"[ë§¤ì§‘ ì¶”ì ê¸°] íƒì§€: {len(results)}ì¢…ëª©")
    for phase in ["í­ë°œ", "ë§¤ì§‘", "ì¬ëŒíŒŒ", "ê°€ì†"]:
        cnt = phase_stats.get(phase, 0)
        if cnt:
            print(f"  {phase}: {cnt}ì¢…ëª©")
    print(f"{'='*60}")

    # ìƒìœ„ 20ê°œ ì¶œë ¥
    top = results[:20]
    if top:
        print(f"\n{'â”€'*65}")
        print(f"  â˜… ì„¸ë ¥ ë§¤ì§‘ ì¶”ì  â€” ìƒìœ„ 20 â˜…")
        print(f"{'â”€'*65}")
        for i, r in enumerate(top, 1):
            phase_icon = {
                "í­ë°œ": "ğŸ’¥",
                "ë§¤ì§‘": "ğŸ”„",
                "ì¬ëŒíŒŒ": "ğŸš€",
                "ê°€ì†": "âš¡",
            }.get(r["phase"], "?")

            print(
                f"  {i:2d}. {phase_icon} [{r['phase']}] {r['name']}({r['ticker']}) "
                f"{r['total_score']}ì  | "
                f"{r['days_since_spike']}ì¼ì „í­ë°œ "
                f"ìˆ˜ìµ:{r['return_since_spike']:+.1f}% "
                f"RSI:{r['rsi']:.0f}"
            )
            print(
                f"      OBV:{r['obv_score']} ìˆ˜ê¸‰:{r['flow_score']} "
                f"ëª¨ë©˜í…€:{r['momentum_score']} êµ¬ì¡°:{r['structure_score']} "
                f"| TRIX:{r['trix']:.3f} ì™¸ì¸ì—°ì†:{r['foreign_consec']}ì¼ "
                f"í­ë°œ{r['n_spikes_60d']}íšŒ"
            )
    else:
        print("  í˜„ì¬ ë§¤ì§‘ ì§„í–‰ ì¤‘ ì¢…ëª© ì—†ìŒ")

    # JSON ì €ì¥
    output = {
        "date": datetime.now().strftime("%Y-%m-%d"),
        "scanned_at": datetime.now().strftime("%Y-%m-%d %H:%M"),
        "total_detected": len(results),
        "phase_stats": phase_stats,
        "top20": [r["ticker"] for r in top],
        "items": results,
    }
    OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2, default=str)

    print(f"\n[ì €ì¥] {OUTPUT_PATH}")


if __name__ == "__main__":
    main()
