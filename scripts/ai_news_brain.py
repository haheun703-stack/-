"""AI ë‘ë‡Œ â€” ë‰´ìŠ¤ ì¢…í•© ë¶„ì„ â†’ ì¢…ëª©ë³„ BUY/WATCH/AVOID íŒë‹¨

ê¸°ì¡´ 5ê°œ ë‰´ìŠ¤ ì†ŒìŠ¤(RSS, Perplexity, Grok, DART, ë„¤ì´ë²„)ì—ì„œ
50~70ê°œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘ â†’ Claude Sonnet 4.6ì´ ì •ì„±ì  ë¶„ì„ â†’ JSON ì¶œë ¥.

ì¶œë ¥: data/ai_brain_judgment.json
ì—°ë™: scan_tomorrow_picks.py (ai_bonus Â±7ì ), send_evening_summary.py

Usage:
    python scripts/ai_news_brain.py           # ë¶„ì„ë§Œ
    python scripts/ai_news_brain.py --send    # ë¶„ì„ + í…”ë ˆê·¸ë¨ ìš”ì•½ ì „ì†¡
"""

from __future__ import annotations

import argparse
import asyncio
import json
import logging
import sys
from datetime import datetime
from difflib import SequenceMatcher
from pathlib import Path

import pandas as pd
import yaml
from dotenv import load_dotenv

PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
load_dotenv(PROJECT_ROOT / ".env")

from src.agents.news_brain import NewsBrainAgent  # noqa: E402

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S",
    handlers=[logging.StreamHandler(sys.stdout)],
)
logger = logging.getLogger(__name__)

DATA_DIR = PROJECT_ROOT / "data"
PROCESSED_DIR = DATA_DIR / "processed"
CSV_DIR = PROJECT_ROOT / "stock_data_daily"
OUTPUT_PATH = DATA_DIR / "ai_brain_judgment.json"
SETTINGS_PATH = PROJECT_ROOT / "config" / "settings.yaml"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„¤ì • ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_settings() -> dict:
    with open(SETTINGS_PATH, encoding="utf-8") as f:
        return yaml.safe_load(f).get("ai_brain", {})


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ ë‹ˆë²„ìŠ¤ ë¹Œë“œ (ticker â†’ name)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_universe() -> dict[str, str]:
    """mechanical_universe_final.csv ê¸°ë°˜ 84ì¢…ëª© ìœ ë‹ˆë²„ìŠ¤ (in_parquet=True)."""
    csv_path = DATA_DIR / "mechanical_universe_final.csv"
    if not csv_path.exists():
        # fallback: processed parquet ì „ì²´
        logger.warning("ìœ ë‹ˆë²„ìŠ¤ CSV ì—†ìŒ â€” processed/*.parquet ì „ì²´ ì‚¬ìš©")
        universe = {}
        for pq in PROCESSED_DIR.glob("*.parquet"):
            ticker = pq.stem
            name = ticker
            for c in CSV_DIR.glob(f"*_{ticker}.csv"):
                parts = c.stem.rsplit("_", 1)
                if len(parts) == 2:
                    name = parts[0]
                    break
            universe[ticker] = name
        return universe

    df = pd.read_csv(csv_path)
    df = df[df["in_parquet"] == True]  # noqa: E712
    universe = {}
    for _, row in df.iterrows():
        ticker = str(row["ticker"]).zfill(6)
        name = str(row["name"])
        universe[ticker] = name
    return universe


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‰´ìŠ¤ ìˆ˜ì§‘ (ê¸°ì¡´ JSON 5ê°œ ì†ŒìŠ¤ í†µí•©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _load_json(name: str) -> dict | list:
    path = DATA_DIR / name
    if path.exists():
        with open(path, encoding="utf-8") as f:
            return json.load(f)
    return {}


def _collect_market_news() -> list[dict]:
    """Google RSS ë‰´ìŠ¤ (market_news.json)."""
    data = _load_json("market_news.json")
    articles = data.get("articles", [])
    result = []
    for a in articles:
        if a.get("impact") in ("high", "medium"):
            result.append({
                "title": a.get("title", ""),
                "summary": "",
                "source": a.get("source", "Google RSS"),
                "category": a.get("category", "ë§¤í¬ë¡œ"),
                "impact": a.get("impact", ""),
            })
    return result


def _collect_intelligence() -> list[dict]:
    """Perplexity ì¸í…”ë¦¬ì „ìŠ¤ (market_intelligence.json)."""
    data = _load_json("market_intelligence.json")
    result = []

    # key_events â†’ ë‰´ìŠ¤ í•­ëª©
    for ev in data.get("key_events", []):
        result.append({
            "title": ev.get("event", ""),
            "summary": ev.get("detail", ""),
            "source": "Perplexity",
            "category": ev.get("category", "ë§¤í¬ë¡œ"),
            "impact": ev.get("urgency", ""),
        })

    # us_market_summary â†’ ìš”ì•½ ë‰´ìŠ¤
    us_summary = data.get("us_market_summary", "")
    if us_summary:
        result.append({
            "title": f"ë¯¸êµ­ì¥ ìš”ì•½: {data.get('us_market_mood', '')}",
            "summary": us_summary[:300],
            "source": "Perplexity",
            "category": "ë¯¸êµ­ì¥",
            "impact": "IMPORTANT",
        })

    # sector_impacts â†’ ì„¹í„°ë³„ ë‰´ìŠ¤
    for sec in data.get("sector_impacts", []):
        result.append({
            "title": f"[{sec.get('sector', '')}] ì„¹í„° ì˜í–¥",
            "summary": sec.get("reason", "")[:200],
            "source": "Perplexity",
            "category": f"ì„¹í„°:{sec.get('sector', '')}",
            "impact": "medium",
        })

    return result


def _collect_dart() -> list[dict]:
    """DART ê³µì‹œ (dart_disclosures.json) â€” tier1ë§Œ."""
    data = _load_json("dart_disclosures.json")
    result = []
    for d in data.get("tier1", []):
        result.append({
            "title": f"[DART ê³µì‹œ] {d.get('corp_name', '')} â€” {d.get('report_nm', '')}",
            "summary": f"í‚¤ì›Œë“œ: {d.get('keyword', '')} | ì‹œì¥: {d.get('market', '')}",
            "source": "DART",
            "category": "ê³µì‹œ",
            "impact": "high",
        })
    return result


def _collect_signal_news() -> list[dict]:
    """ë„¤ì´ë²„ í¬ë¡¤ë§ ë‰´ìŠ¤ (signal_news.json)."""
    data = _load_json("signal_news.json")
    if isinstance(data, list):
        items = data
    elif isinstance(data, dict):
        items = data.get("news", data.get("items", []))
    else:
        return []
    result = []
    for item in items:
        if not isinstance(item, dict):
            continue
        result.append({
            "title": item.get("title", ""),
            "summary": item.get("summary", item.get("description", "")),
            "source": item.get("source", "ë„¤ì´ë²„"),
            "category": item.get("category", item.get("classification", "ì¢…ëª©")),
            "impact": item.get("impact", "medium"),
        })
    return result


def _collect_grok_news() -> list[dict]:
    """Grok ë‰´ìŠ¤ (data/grok_news_*.json)."""
    result = []
    for gf in sorted(DATA_DIR.glob("grok_news_*.json")):
        try:
            with open(gf, encoding="utf-8") as f:
                data = json.load(f)
        except Exception:
            continue
        items = data if isinstance(data, list) else data.get("news", [])
        for item in items:
            if not isinstance(item, dict):
                continue
            result.append({
                "title": item.get("title", item.get("headline", "")),
                "summary": item.get("summary", item.get("analysis", "")),
                "source": item.get("source", "Grok"),
                "category": item.get("category", "ì¢…ëª©"),
                "impact": item.get("impact", "medium"),
            })
    return result


def _deduplicate(news: list[dict], threshold: float = 0.7) -> list[dict]:
    """ì œëª© ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°."""
    unique = []
    seen_titles = []
    for item in news:
        title = item.get("title", "")
        if not title:
            continue
        is_dup = False
        for seen in seen_titles:
            if SequenceMatcher(None, title, seen).ratio() > threshold:
                is_dup = True
                break
        if not is_dup:
            unique.append(item)
            seen_titles.append(title)
    return unique


def collect_all_news(max_items: int = 70) -> list[dict]:
    """5ê°œ ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ í†µí•© ìˆ˜ì§‘."""
    all_news = []

    # ì†ŒìŠ¤ë³„ ìˆ˜ì§‘ (ìš°ì„ ìˆœìœ„: DART > Perplexity > RSS > Grok > ë„¤ì´ë²„)
    dart_news = _collect_dart()
    intel_news = _collect_intelligence()
    market_news = _collect_market_news()
    grok_news = _collect_grok_news()
    signal_news = _collect_signal_news()

    logger.info(
        "ë‰´ìŠ¤ ìˆ˜ì§‘: DART=%d, Perplexity=%d, RSS=%d, Grok=%d, ë„¤ì´ë²„=%d",
        len(dart_news), len(intel_news), len(market_news),
        len(grok_news), len(signal_news),
    )

    # ìš°ì„ ìˆœìœ„ ìˆœ ë³‘í•©
    all_news.extend(dart_news)
    all_news.extend(intel_news)
    all_news.extend(market_news)
    all_news.extend(grok_news)
    all_news.extend(signal_news)

    # ì¤‘ë³µ ì œê±°
    unique = _deduplicate(all_news)
    logger.info("ì¤‘ë³µ ì œê±°: %d â†’ %dê±´", len(all_news), len(unique))

    # ìµœëŒ€ ê°œìˆ˜ ì œí•œ
    if len(unique) > max_items:
        unique = unique[:max_items]

    return unique


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì±„ë„ 4: í”¼ë“œë°± ë£¨í”„ â€” ì´ì „ íŒë‹¨ ì•„ì¹´ì´ë¸Œ + ìˆ˜ìµë¥  ì¶”ì 
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

HISTORY_PATH = DATA_DIR / "ai_brain_history.json"


def _archive_previous_judgment():
    """ì´ì „ AI íŒë‹¨ì„ íˆìŠ¤í† ë¦¬ì— ì•„ì¹´ì´ë¸Œí•˜ê³  D+3/5/7 ìˆ˜ìµë¥  ì¶”ì ."""
    if not OUTPUT_PATH.exists():
        return

    try:
        prev = json.loads(OUTPUT_PATH.read_text(encoding="utf-8"))
    except Exception:
        return

    prev_date = prev.get("date", "")
    if not prev_date:
        return

    # íˆìŠ¤í† ë¦¬ ë¡œë“œ/ìƒì„±
    history = []
    if HISTORY_PATH.exists():
        try:
            history = json.loads(HISTORY_PATH.read_text(encoding="utf-8"))
        except Exception:
            history = []

    # ì¤‘ë³µ ë°©ì§€
    existing_dates = {h.get("date") for h in history}
    if prev_date not in existing_dates:
        history.append(prev)
        logger.info("[ì•„ì¹´ì´ë¸Œ] %s AI íŒë‹¨ ì €ì¥ (%dê±´)",
                     prev_date, len(prev.get("stock_judgments", [])))

    # D+3/5/7 ìˆ˜ìµë¥  ì¶”ì 
    _update_tracking_returns(history)

    # ìµœëŒ€ 90ì¼ë¶„ ìœ ì§€
    history = history[-90:]
    HISTORY_PATH.write_text(
        json.dumps(history, ensure_ascii=False, indent=2), encoding="utf-8"
    )


def _update_tracking_returns(history: list):
    """ê³¼ê±° AI BUY íŒë‹¨ì˜ D+3/5/7 ìˆ˜ìµë¥  ì¶”ì ."""
    today = datetime.now().date()

    for entry in history:
        if entry.get("tracking_complete"):
            continue

        entry_date_str = entry.get("date", "")
        if not entry_date_str:
            continue
        try:
            entry_date = datetime.strptime(entry_date_str, "%Y-%m-%d").date()
        except ValueError:
            continue

        days_elapsed = (today - entry_date).days
        if days_elapsed < 3:
            continue

        for j in entry.get("stock_judgments", []):
            if j.get("action") != "BUY":
                continue
            ticker = j.get("ticker", "")
            if not ticker:
                continue

            pq_path = PROCESSED_DIR / f"{ticker}.parquet"
            if not pq_path.exists():
                continue

            try:
                df = pd.read_parquet(pq_path)
                if not isinstance(df.index, pd.DatetimeIndex):
                    df.index = pd.to_datetime(df.index)

                mask = df.index.date >= entry_date
                if mask.sum() < 2:
                    continue
                sub = df[mask]
                base_price = float(sub.iloc[0]["close"])

                for d in [3, 5, 7]:
                    key = f"ret_d{d}"
                    if key in j:
                        continue
                    if len(sub) > d:
                        future_price = float(sub.iloc[d]["close"])
                        j[key] = round((future_price / base_price - 1) * 100, 2)
            except Exception:
                continue

        if days_elapsed >= 7:
            entry["tracking_complete"] = True


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ ë¶„ì„ ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def run_daily_analysis(cfg: dict) -> dict:
    """ë‰´ìŠ¤ ìˆ˜ì§‘ â†’ AI ë¶„ì„ â†’ JSON ì €ì¥."""
    if not cfg.get("enabled", True):
        logger.info("AI Brain ë¹„í™œì„±í™” â€” ìŠ¤í‚µ")
        return {}

    # 0. ì´ì „ AI íŒë‹¨ ì•„ì¹´ì´ë¸Œ + ìˆ˜ìµë¥  ì¶”ì 
    _archive_previous_judgment()

    max_items = cfg.get("max_news_items", 70)
    max_judgments = cfg.get("max_stock_judgments", 50)
    model = cfg.get("model", "claude-sonnet-4-5-20250929")

    # 1. ë‰´ìŠ¤ ìˆ˜ì§‘
    news = collect_all_news(max_items=max_items)
    if len(news) < 5:
        logger.warning("ë‰´ìŠ¤ %dê±´ â€” ë¶„ì„ ë¶ˆì¶©ë¶„, ìŠ¤í‚µ", len(news))
        return {}

    # 2. ìœ ë‹ˆë²„ìŠ¤ ë¹Œë“œ
    universe = build_universe()
    logger.info("ìœ ë‹ˆë²„ìŠ¤: %dì¢…ëª©, ìµœëŒ€ íŒë‹¨: %dê°œ", len(universe), max_judgments)

    # 3. AI ë¶„ì„
    agent = NewsBrainAgent(model=model)
    result = await agent.analyze_daily_news(news, universe, max_judgments=max_judgments)

    # 4. ë©”íƒ€ë°ì´í„° ì¶”ê°€
    result["date"] = datetime.now().strftime("%Y-%m-%d")
    result["generated_at"] = datetime.now().strftime("%Y-%m-%d %H:%M")
    result["model"] = model
    result["news_count"] = len(news)

    # 5. JSON ì €ì¥
    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    logger.info("ì €ì¥ ì™„ë£Œ: %s", OUTPUT_PATH)

    return result


def _print_summary(result: dict) -> None:
    """ë¶„ì„ ê²°ê³¼ ì½˜ì†” ìš”ì•½."""
    if not result:
        return

    print("\n" + "=" * 60)
    print("ğŸ§  AI ë‘ë‡Œ ë¶„ì„ ê²°ê³¼")
    print("=" * 60)
    print(f"ë‚ ì§œ: {result.get('date', '?')}")
    print(f"ì‹œì¥ ì„¼í‹°ë¨¼íŠ¸: {result.get('market_sentiment', '?')}")
    themes = result.get("key_themes", [])
    if themes:
        print(f"í•µì‹¬ í…Œë§ˆ: {', '.join(themes)}")

    judgments = result.get("stock_judgments", [])
    if judgments:
        print(f"\nì¢…ëª© íŒë‹¨: {len(judgments)}ê°œ")
        print("-" * 50)
        for j in judgments:
            action = j.get("action", "?")
            icon = {"BUY": "ğŸŸ¢", "WATCH": "ğŸŸ¡", "AVOID": "ğŸ”´"}.get(action, "âšª")
            conf = j.get("confidence", 0)
            print(
                f"  {icon} {j.get('name', '?'):10s} | {action:5s} "
                f"({conf:.0%}) | {j.get('reasoning', '')[:40]}"
            )

    sector = result.get("sector_outlook", {})
    if sector:
        print(f"\nì„¹í„° ì „ë§: {len(sector)}ê°œ")
        for s, info in sector.items():
            d = info.get("direction", "?") if isinstance(info, dict) else "?"
            print(f"  {s}: {d}")

    print(f"\në¶„ì„ ë‰´ìŠ¤: {result.get('news_count', 0)}ê±´")
    print("=" * 60)


def _send_telegram(result: dict) -> None:
    """í•µì‹¬ ìš”ì•½ì„ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡."""
    try:
        from src.telegram_sender import send_message
    except ImportError:
        logger.warning("telegram_sender ì„í¬íŠ¸ ì‹¤íŒ¨ â€” ì „ì†¡ ìŠ¤í‚µ")
        return

    if not result or not result.get("stock_judgments"):
        return

    lines = ["ğŸ§  AI ë‘ë‡Œ ì¼ì¼ ë¶„ì„"]
    lines.append(f"ì„¼í‹°ë¨¼íŠ¸: {result.get('market_sentiment', '?')}")
    themes = result.get("key_themes", [])
    if themes:
        lines.append(f"í…Œë§ˆ: {', '.join(themes[:3])}")
    lines.append("")

    for j in result.get("stock_judgments", []):
        action = j.get("action", "?")
        icon = {"BUY": "ğŸŸ¢", "WATCH": "ğŸŸ¡", "AVOID": "ğŸ”´"}.get(action, "âšª")
        conf = j.get("confidence", 0)
        lines.append(
            f"{icon} {j.get('name', '?')} {action}({conf:.0%})"
        )
        lines.append(f"  â†’ {j.get('reasoning', '')[:50]}")

    msg = "\n".join(lines)
    send_message(msg)
    logger.info("í…”ë ˆê·¸ë¨ ì „ì†¡ ì™„ë£Œ")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    parser = argparse.ArgumentParser(description="AI ë‘ë‡Œ ë‰´ìŠ¤ ë¶„ì„")
    parser.add_argument("--send", action="store_true", help="í…”ë ˆê·¸ë¨ ì „ì†¡")
    args = parser.parse_args()

    cfg = load_settings()
    result = asyncio.run(run_daily_analysis(cfg))

    _print_summary(result)

    if args.send and result:
        _send_telegram(result)


if __name__ == "__main__":
    main()
