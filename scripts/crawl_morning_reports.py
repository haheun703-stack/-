"""ì¥ì „ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ìŠ¤ìº” â€” ë„¤ì´ë²„ì¦ê¶Œ ë¦¬ì„œì¹˜ + Perplexity í…Œë§ˆ ì˜ˆì¸¡

07:00 BAT-Bì—ì„œ í˜¸ì¶œ. ë‹¹ì¼ ë°œí–‰ ë¦¬í¬íŠ¸ì—ì„œ ëª©í‘œê°€ ìƒí–¥/íˆ¬ìì˜ê²¬ ë³€ê²½ ê°ì§€,
Perplexityì— ì¥ì „ ê¸‰ë“± í…Œë§ˆ ì§ˆë¬¸, í›„ë³´í’€ ì¢…ëª© ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§.

ì¶œë ¥: data/morning_reports.json
ì—°ë™: scan_tomorrow_picks.pyì—ì„œ report_bonus (ì „ëµ G) ë°˜ì˜

Usage:
    python scripts/crawl_morning_reports.py              # ê¸°ë³¸ (ë¯¸ë¦¬ë³´ê¸°)
    python scripts/crawl_morning_reports.py --send        # í…”ë ˆê·¸ë¨ ë°œì†¡
    python scripts/crawl_morning_reports.py --no-perplexity  # Perplexity ìŠ¤í‚µ
    python scripts/crawl_morning_reports.py --no-news     # ë‰´ìŠ¤ ìŠ¤ìº” ìŠ¤í‚µ
"""

from __future__ import annotations

import argparse
import json
import logging
import re
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv

PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
load_dotenv(PROJECT_ROOT / ".env")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S",
    handlers=[logging.StreamHandler(sys.stdout)],
)
logger = logging.getLogger(__name__)

DATA_DIR = PROJECT_ROOT / "data"
OUTPUT_PATH = DATA_DIR / "morning_reports.json"

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Referer": "https://finance.naver.com/",
}

NAVER_RESEARCH_URL = "https://finance.naver.com/research/company_list.naver"

# â”€â”€ ë¦¬í¬íŠ¸ íˆ¬ìì˜ê²¬ ë¶„ë¥˜ â”€â”€
POSITIVE_OPINIONS = {"ë§¤ìˆ˜", "BUY", "Strong Buy", "Trading BUY", "Outperform", "ë¹„ì¤‘í™•ëŒ€"}
NEGATIVE_OPINIONS = {"ë§¤ë„", "SELL", "Underperform", "ë¹„ì¤‘ì¶•ì†Œ", "ì¤‘ë¦½"}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  1. ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ í¬ë¡¤ë§
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _fetch_detail_opinion(detail_url: str) -> tuple[str, int]:
    """ìƒì„¸ í˜ì´ì§€ì—ì„œ ëª©í‘œê°€ + íˆ¬ìì˜ê²¬ ì¶”ì¶œ.

    Returns:
        (opinion, target_price) â€” ì˜ˆ: ("Buy", 90000)
    """
    try:
        resp = requests.get(detail_url, headers=HEADERS, timeout=10)
        resp.encoding = "euc-kr"
        soup = BeautifulSoup(resp.text, "html.parser")

        # ìƒì„¸ í˜ì´ì§€ì˜ type_1 í…Œì´ë¸”ì—ì„œ "ëª©í‘œê°€...|íˆ¬ìì˜ê²¬..." í˜•íƒœ íŒŒì‹±
        table = soup.select_one("table.type_1")
        if not table:
            return "", 0

        text = table.get_text(" ", strip=True)
        # ëª©í‘œê°€ ì¶”ì¶œ: "ëª©í‘œê°€90,000" or "ëª©í‘œê°€ 90,000"
        tp_match = re.search(r"ëª©í‘œê°€\s*([0-9,]+)", text)
        target_price = int(tp_match.group(1).replace(",", "")) if tp_match else 0
        # íˆ¬ìì˜ê²¬ ì¶”ì¶œ: "íˆ¬ìì˜ê²¬Buy" or "íˆ¬ìì˜ê²¬ ë§¤ìˆ˜"
        op_match = re.search(
            r"íˆ¬ìì˜ê²¬\s*(\w+)",
            text,
        )
        opinion = op_match.group(1) if op_match else ""
        return opinion, target_price

    except Exception:
        return "", 0


def fetch_naver_research(max_pages: int = 3) -> list[dict]:
    """ë„¤ì´ë²„ì¦ê¶Œ ë¦¬ì„œì¹˜ì—ì„œ ë‹¹ì¼+ì „ì¼ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ìˆ˜ì§‘.

    ì‹¤ì œ HTML êµ¬ì¡° (table.type_1):
      ì»¬ëŸ¼: ì¢…ëª©ëª… | ì œëª© | ì¦ê¶Œì‚¬ | ì²¨ë¶€ | ì‘ì„±ì¼ | ì¡°íšŒìˆ˜
      ì¸ë±ìŠ¤: td[0]  td[1]  td[2]   td[3]  td[4]   td[5]

    ëª©í‘œê°€/íˆ¬ìì˜ê²¬ì€ ëª©ë¡ì— ì—†ìŒ â†’ ìƒì„¸ í˜ì´ì§€(company_read)ì—ì„œ ì¶”ì¶œ.

    Returns:
        [{"stock_name", "stock_code", "title", "broker",
          "target_price", "opinion", "date", "url"}, ...]
    """
    today_str = datetime.now().strftime("%y.%m.%d")
    yesterday_str = (datetime.now() - timedelta(days=1)).strftime("%y.%m.%d")
    valid_dates = {today_str, yesterday_str}

    reports: list[dict] = []
    seen: set[str] = set()

    for page in range(1, max_pages + 1):
        try:
            resp = requests.get(
                NAVER_RESEARCH_URL,
                params={"page": page},
                headers=HEADERS,
                timeout=15,
            )
            resp.raise_for_status()
            resp.encoding = "euc-kr"
            soup = BeautifulSoup(resp.text, "html.parser")

            table = soup.select_one("table.type_1")
            if not table:
                logger.warning("í…Œì´ë¸” ë¯¸ë°œê²¬ (page %d)", page)
                break

            rows = table.select("tr")
            found_old = False
            for row in rows:
                tds = row.select("td")
                if len(tds) < 6:
                    continue

                # ì‘ì„±ì¼ì€ td[4] (ì¸ë±ìŠ¤ 4)
                date_text = tds[4].get_text(strip=True)
                if date_text not in valid_dates:
                    if date_text and date_text < yesterday_str:
                        found_old = True
                    continue

                # td[0]: ì¢…ëª©ëª… + ì½”ë“œ
                stock_link = tds[0].select_one("a")
                if not stock_link:
                    continue
                stock_name = stock_link.get_text(strip=True)
                href = stock_link.get("href", "")
                code_match = re.search(r"code=(\d{6})", href)
                stock_code = code_match.group(1) if code_match else ""

                # td[1]: ë¦¬í¬íŠ¸ ì œëª© + ìƒì„¸ URL (nid)
                title_link = tds[1].select_one("a")
                title = title_link.get_text(strip=True) if title_link else ""
                detail_url = ""
                if title_link:
                    href2 = title_link.get("href", "")
                    if href2 and not href2.startswith("http"):
                        detail_url = "https://finance.naver.com/research/" + href2
                    else:
                        detail_url = href2

                # td[2]: ì¦ê¶Œì‚¬
                broker = tds[2].get_text(strip=True)

                # ì¤‘ë³µ ì œê±°
                key = f"{stock_code}_{broker}_{date_text}"
                if key in seen:
                    continue
                seen.add(key)

                reports.append({
                    "stock_name": stock_name,
                    "stock_code": stock_code,
                    "title": title,
                    "broker": broker,
                    "target_price": 0,
                    "opinion": "",
                    "date": date_text,
                    "url": detail_url,
                })

            if found_old:
                break

            time.sleep(1.0)

        except Exception as e:
            logger.warning("ë¦¬ì„œì¹˜ í¬ë¡¤ë§ ì‹¤íŒ¨ (page %d): %s", page, e)
            break

    # 2ë‹¨ê³„: ê° ë¦¬í¬íŠ¸ ìƒì„¸ í˜ì´ì§€ì—ì„œ ëª©í‘œê°€/íˆ¬ìì˜ê²¬ ì¶”ì¶œ
    if reports:
        logger.info("ìƒì„¸ í˜ì´ì§€ %dê±´ ì¡°íšŒ ì¤‘...", len(reports))
        for i, r in enumerate(reports):
            if r["url"]:
                opinion, target = _fetch_detail_opinion(r["url"])
                r["opinion"] = opinion
                r["target_price"] = target
                if (i + 1) % 5 == 0:
                    logger.info("  %d/%d ì™„ë£Œ", i + 1, len(reports))
                time.sleep(0.5)

    logger.info("ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ %dê±´ ìˆ˜ì§‘", len(reports))
    return reports


def classify_reports(reports: list[dict]) -> list[dict]:
    """ë¦¬í¬íŠ¸ë¥¼ ì˜í–¥ë„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜í•˜ê³  boost ì ìˆ˜ ì‚°ì •."""
    for r in reports:
        opinion = r.get("opinion", "")
        title = r.get("title", "")

        # ì œëª© í‚¤ì›Œë“œ ê°ì§€
        is_upgrade = bool(re.search(
            r"ìƒí–¥|ì—…ê·¸ë ˆì´ë“œ|ê¸ì •|ì„œí”„ë¼ì´ì¦ˆ|ëª©í‘œê°€.*ì¸ìƒ|ì‹¤ì .*í˜¸ì „|ìˆ˜ì£¼.*í™•ëŒ€|ì„±ì¥.*ê°€ì†",
            title,
        ))
        is_downgrade = bool(re.search(
            r"í•˜í–¥|ë‹¤ìš´ê·¸ë ˆì´ë“œ|ë¶€ì •|ì‹¤ì .*ë¶€ì§„|ë‘”í™”|ë¦¬ìŠ¤í¬.*í™•ëŒ€|ëª©í‘œê°€.*ì¸í•˜",
            title,
        ))

        is_buy = any(op in opinion for op in POSITIVE_OPINIONS)
        is_sell = any(op in opinion for op in NEGATIVE_OPINIONS)

        if is_buy and is_upgrade:
            boost, tag = 8, "ëª©í‘œê°€ìƒí–¥+ë§¤ìˆ˜"
        elif is_buy:
            boost, tag = 5, "ë§¤ìˆ˜"
        elif is_sell or is_downgrade:
            boost, tag = -5, "ë¹„ì¤‘ì¶•ì†Œ"
        else:
            boost, tag = 0, "ì¤‘ë¦½"

        r["boost"] = boost
        r["tag"] = tag

    return reports


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  2. Perplexity ì¥ì „ í…Œë§ˆ ì˜ˆì¸¡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def query_perplexity_morning_theme() -> dict | None:
    """Perplexityì— 'ì˜¤ëŠ˜ í•œêµ­ì¥ ê¸‰ë“± ì˜ˆìƒ í…Œë§ˆ/ì¢…ëª©' ì§ˆë¬¸ (1íšŒ í˜¸ì¶œ)."""
    try:
        from scripts.perplexity_market_intel import query_perplexity
    except ImportError:
        logger.warning("perplexity_market_intel import ì‹¤íŒ¨")
        return None

    today = datetime.now().strftime("%Y-%m-%d")
    prompt = f"""ì˜¤ëŠ˜ ë‚ ì§œ: {today}

ì˜¤ëŠ˜ í•œêµ­ ì£¼ì‹ì‹œì¥ì—ì„œ ì£¼ëª©í•´ì•¼ í•  ê¸‰ë“± ê°€ëŠ¥ í…Œë§ˆì™€ ì¢…ëª©ì„ ë¶„ì„í•˜ì„¸ìš”.
ë‹¤ìŒ JSON êµ¬ì¡°ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:

{{
  "hot_themes": [
    {{
      "theme": "í…Œë§ˆëª…",
      "catalyst": "ì´‰ë§¤ ì´ìœ  1ë¬¸ì¥",
      "urgency": "HIGH|MEDIUM|LOW",
      "stocks": ["ì¢…ëª©ëª…1", "ì¢…ëª©ëª…2", "ì¢…ëª©ëª…3"]
    }}
  ],
  "breaking_catalysts": [
    {{
      "stock_name": "ì¢…ëª©ëª…",
      "catalyst": "ì´‰ë§¤ ë‚´ìš© (ìˆ˜ì£¼/FDA/M&A/ì‹¤ì /ë¦¬í¬íŠ¸ ë“±)",
      "impact": "positive|negative",
      "source": "ë‰´ìŠ¤ ì¶œì²˜"
    }}
  ],
  "sector_outlook": {{
    "bullish": ["ìƒìŠ¹ ì˜ˆìƒ ì„¹í„° ìµœëŒ€ 3ê°œ"],
    "bearish": ["í•˜ë½ ì£¼ì˜ ì„¹í„° ìµœëŒ€ 2ê°œ"]
  }}
}}

hot_themesëŠ” ì¤‘ìš”ë„ìˆœ ìµœëŒ€ 5ê°œ.
breaking_catalystsëŠ” ê°œë³„ ì¢…ëª© ì´‰ë§¤ ë‰´ìŠ¤ ìµœëŒ€ 10ê°œ.
í•œêµ­ ì¥ì „ ì‹œì ì˜ ìµœì‹  ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.
ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ë°œí–‰, ìˆ˜ì£¼/ê³„ì•½ ê³µì‹œ, FDA ìŠ¹ì¸, ì •ì±… ìˆ˜í˜œ ë“± êµ¬ì²´ì  ì´‰ë§¤ë§Œ í¬í•¨í•˜ì„¸ìš”."""

    result = query_perplexity(prompt)
    if result and result.get("parse_error"):
        logger.warning("Perplexity JSON íŒŒì‹± ì‹¤íŒ¨")
        return None
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  3. í›„ë³´í’€ ì¢…ëª© ë‰´ìŠ¤ ì²´í¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _build_name_map() -> dict[str, str]:
    """ticker â†’ ì¢…ëª©ëª… ë§¤í•‘ (tomorrow_picks.jsonì—ì„œ)."""
    try:
        with open(DATA_DIR / "tomorrow_picks.json", "r", encoding="utf-8") as f:
            picks = json.load(f)
        return {p["ticker"]: p["name"] for p in picks.get("picks", []) if p.get("name")}
    except Exception:
        return {}


def scan_candidate_news(tickers: list[str], name_map: dict[str, str]) -> list[dict]:
    """í›„ë³´í’€ ì¢…ëª© ì¤‘ 24h ë‚´ ë‰´ìŠ¤ í„°ì§„ ì¢…ëª© ê°ì§€.

    ìµœëŒ€ 30ì¢…ëª©ë§Œ ê²€ìƒ‰ (rate limit).
    """
    try:
        from scripts.crawl_market_news import crawl_stock_news
    except ImportError:
        logger.warning("crawl_market_news import ì‹¤íŒ¨")
        return []

    stock_names = [name_map[t] for t in tickers[:30] if t in name_map]
    if not stock_names:
        return []

    news = crawl_stock_news(stock_names, days=1)
    # high/mediumë§Œ í•„í„°
    return [n for n in news if n.get("impact") in ("high", "medium")]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  4. ìµœì¢… ì¶œë ¥ + í…”ë ˆê·¸ë¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_morning_output(
    reports: list[dict],
    perplexity_result: dict | None,
    candidate_news: list[dict],
) -> dict:
    """morning_reports.json ìµœì¢… êµ¬ì„±."""
    now = datetime.now()

    # report_boost_map: ticker â†’ {boost, tag, broker, target_price, title}
    report_boost_map: dict[str, dict] = {}
    for r in reports:
        code = r.get("stock_code", "")
        if code and r.get("boost", 0) != 0:
            existing = report_boost_map.get(code, {})
            if abs(r["boost"]) > abs(existing.get("boost", 0)):
                report_boost_map[code] = {
                    "boost": r["boost"],
                    "tag": r["tag"],
                    "broker": r["broker"],
                    "target_price": r["target_price"],
                    "title": r["title"][:50],
                }

    # news_boost_map: ì¢…ëª©ëª… â†’ {boost, reason}
    news_boost_map: dict[str, dict] = {}
    for n in candidate_news:
        sname = n.get("stock_name", "")
        if not sname:
            continue
        if n.get("impact") == "high":
            news_boost_map[sname] = {"boost": 5, "reason": n.get("title", "")[:40]}
        elif sname not in news_boost_map:
            news_boost_map[sname] = {"boost": 3, "reason": n.get("title", "")[:40]}

    return {
        "date": now.strftime("%Y-%m-%d"),
        "generated_at": now.strftime("%Y-%m-%d %H:%M"),
        "reports": reports,
        "report_boost_map": report_boost_map,
        "perplexity_themes": perplexity_result or {},
        "candidate_news": candidate_news,
        "news_boost_map": news_boost_map,
        "stats": {
            "total_reports": len(reports),
            "positive_reports": len([r for r in reports if r.get("boost", 0) > 0]),
            "negative_reports": len([r for r in reports if r.get("boost", 0) < 0]),
            "news_alerts": len(candidate_news),
            "themes_count": len((perplexity_result or {}).get("hot_themes", [])),
        },
    }


def build_telegram_alert(output: dict) -> str:
    """ì¥ì „ ë¦¬í¬íŠ¸ ìŠ¤ìº” í…”ë ˆê·¸ë¨ ë©”ì‹œì§€."""
    date = output["date"]
    stats = output["stats"]
    reports = output.get("reports", [])

    lines = [
        f"ğŸ“‹ ì¥ì „ ë¦¬í¬íŠ¸ ìŠ¤ìº” ({date})",
        "â”" * 28,
    ]

    # ë§¤ìˆ˜ ë¦¬í¬íŠ¸
    positive = [r for r in reports if r.get("boost", 0) > 0]
    if positive:
        lines.append(f"\nâœ… ë§¤ìˆ˜ ë¦¬í¬íŠ¸ ({len(positive)}ê±´)")
        for r in positive[:8]:
            tp = f" ëª©í‘œ:{r['target_price']:,}" if r.get("target_price") else ""
            lines.append(f"  ğŸ“Œ {r['stock_name']}({r['stock_code']}) â€” {r['broker']}")
            lines.append(f"     {r['tag']}{tp}")

    # ì£¼ì˜ ë¦¬í¬íŠ¸
    negative = [r for r in reports if r.get("boost", 0) < 0]
    if negative:
        lines.append(f"\nâš ï¸ ì£¼ì˜ ë¦¬í¬íŠ¸ ({len(negative)}ê±´)")
        for r in negative[:3]:
            lines.append(f"  ğŸ”» {r['stock_name']}({r['stock_code']}) â€” {r['broker']} [{r['tag']}]")

    # Perplexity í…Œë§ˆ
    pplx = output.get("perplexity_themes", {})
    themes = pplx.get("hot_themes", [])
    if themes:
        lines.append(f"\nğŸ”¥ ì˜¤ëŠ˜ ì£¼ëª© í…Œë§ˆ")
        for t in themes[:4]:
            icon = {"HIGH": "ğŸš¨", "MEDIUM": "âš¡", "LOW": "ğŸ“Œ"}.get(t.get("urgency", ""), "ğŸ“Œ")
            stocks_str = ", ".join(t.get("stocks", [])[:3])
            lines.append(f"  {icon} {t.get('theme', '')}")
            if t.get("catalyst"):
                lines.append(f"     {t['catalyst']}")
            if stocks_str:
                lines.append(f"     â†’ {stocks_str}")

    # ê°œë³„ ì´‰ë§¤
    catalysts = pplx.get("breaking_catalysts", [])
    if catalysts:
        lines.append(f"\nğŸ’¥ ê°œë³„ ì´‰ë§¤ ({len(catalysts)}ê±´)")
        for c in catalysts[:5]:
            icon = "ğŸŸ¢" if c.get("impact") == "positive" else "ğŸ”´"
            lines.append(f"  {icon} {c.get('stock_name', '')} â€” {c.get('catalyst', '')[:40]}")

    # í›„ë³´í’€ ë‰´ìŠ¤
    news = output.get("candidate_news", [])
    if news:
        lines.append(f"\nğŸ“° í›„ë³´í’€ ë‰´ìŠ¤ ({len(news)}ê±´)")
        for n in news[:5]:
            lines.append(f"  â€¢ [{n.get('stock_name', '')}] {n.get('title', '')[:35]}")

    lines.append(
        f"\nğŸ“Š ë¦¬í¬íŠ¸ {stats['total_reports']}ê±´ | "
        f"ë§¤ìˆ˜ {stats['positive_reports']} | "
        f"ì£¼ì˜ {stats['negative_reports']} | "
        f"í…Œë§ˆ {stats['themes_count']}"
    )

    return "\n".join(lines)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  main
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    parser = argparse.ArgumentParser(description="ì¥ì „ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ìŠ¤ìº”")
    parser.add_argument("--send", action="store_true", help="í…”ë ˆê·¸ë¨ ì „ì†¡")
    parser.add_argument("--no-perplexity", action="store_true", help="Perplexity ìŠ¤í‚µ")
    parser.add_argument("--no-news", action="store_true", help="ë‰´ìŠ¤ ìŠ¤ìº” ìŠ¤í‚µ")
    args = parser.parse_args()

    print("=" * 55)
    print("ğŸ“‹ ì¥ì „ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ìŠ¤ìº” ì‹œì‘")
    print("=" * 55)

    # 1. ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸
    print("\n[1/3] ë„¤ì´ë²„ ë¦¬ì„œì¹˜ í¬ë¡¤ë§...")
    reports = fetch_naver_research(max_pages=3)
    reports = classify_reports(reports)
    pos = len([r for r in reports if r.get("boost", 0) > 0])
    neg = len([r for r in reports if r.get("boost", 0) < 0])
    print(f"  â†’ {len(reports)}ê±´ (ë§¤ìˆ˜ {pos} | ì£¼ì˜ {neg})")

    # 2. Perplexity ì¥ì „ í…Œë§ˆ
    pplx_result = None
    if not args.no_perplexity:
        print("\n[2/3] Perplexity ì¥ì „ í…Œë§ˆ ì˜ˆì¸¡...")
        pplx_result = query_perplexity_morning_theme()
        if pplx_result:
            themes = pplx_result.get("hot_themes", [])
            cats = pplx_result.get("breaking_catalysts", [])
            print(f"  â†’ í…Œë§ˆ {len(themes)}ê°œ | ì´‰ë§¤ {len(cats)}ê±´")
        else:
            print("  â†’ Perplexity ì‹¤íŒ¨ â€” ìŠ¤í‚µ")
    else:
        print("\n[2/3] Perplexity ìŠ¤í‚µ (--no-perplexity)")

    # 3. í›„ë³´í’€ ë‰´ìŠ¤
    candidate_news: list[dict] = []
    if not args.no_news:
        print("\n[3/3] í›„ë³´í’€ ë‰´ìŠ¤ ìŠ¤ìº”...")
        name_map = _build_name_map()
        # ê´€ì°° ë“±ê¸‰ ì´ìƒ ì¢…ëª©ë§Œ
        try:
            with open(DATA_DIR / "tomorrow_picks.json", "r", encoding="utf-8") as f:
                picks = json.load(f)
            watchable = [
                p["ticker"]
                for p in picks.get("picks", [])
                if p.get("grade") in ("ì ê·¹ë§¤ìˆ˜", "ë§¤ìˆ˜", "ê´€ì‹¬ë§¤ìˆ˜", "ê´€ì°°")
            ]
        except Exception:
            watchable = []
        if watchable:
            candidate_news = scan_candidate_news(watchable, name_map)
            print(f"  â†’ ë‰´ìŠ¤ {len(candidate_news)}ê±´ ê°ì§€")
        else:
            print("  â†’ í›„ë³´í’€ ì—†ìŒ â€” ìŠ¤í‚µ")
    else:
        print("\n[3/3] ë‰´ìŠ¤ ìŠ¤í‚µ (--no-news)")

    # ìµœì¢… ì¶œë ¥
    output = build_morning_output(reports, pplx_result, candidate_news)

    DATA_DIR.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    print(f"\n[ì €ì¥] {OUTPUT_PATH}")

    # í…”ë ˆê·¸ë¨
    msg = build_telegram_alert(output)
    if args.send:
        try:
            from src.telegram_sender import send_message
            ok = send_message(msg)
            print(f"[í…”ë ˆê·¸ë¨] ì „ì†¡ {'ì„±ê³µ' if ok else 'ì‹¤íŒ¨'}")
        except Exception as e:
            print(f"[í…”ë ˆê·¸ë¨] ì „ì†¡ ì‹¤íŒ¨: {e}")
    else:
        print("\n[ë¯¸ë¦¬ë³´ê¸°]")
        print(msg)

    print(f"\nğŸ“Š ìµœì¢…: ë¦¬í¬íŠ¸ {output['stats']['total_reports']}ê±´ | "
          f"ë§¤ìˆ˜ {output['stats']['positive_reports']} | "
          f"í…Œë§ˆ {output['stats']['themes_count']}")


if __name__ == "__main__":
    main()
